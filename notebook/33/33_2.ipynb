{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sharing code train this dataset as a regression task.\n",
    "\n",
    "But in this code, I train as a classification task.\n",
    "\n",
    "I encode the target value pressure to 950 classes and calculate CrossEntropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# secret_label = \"wandb\"\n",
    "# secret_value = UserSecretsClient().get_secret(secret_label)\n",
    "# # !wandb login $secret_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "# import wandb\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../src/')\n",
    "import utils as utils\n",
    "from utils import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    EXP_NAME = \"exp068_categorical\"\n",
    "    \n",
    "    INPUT = \"/home/knikaido/work/Ventilator-Pressure-Prediction/data/ventilator-pressure-prediction\"\n",
    "    OUTPUT = \"./output\"\n",
    "    N_FOLD = 5\n",
    "    SEED = 0\n",
    "    \n",
    "    LR = 5e-3\n",
    "    N_EPOCHS = 50\n",
    "    EMBED_SIZE = 64\n",
    "    HIDDEN_SIZE = 256\n",
    "    BS = 512\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "\n",
    "    USE_LAG = 4\n",
    "    CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n",
    "    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2']\n",
    "    LAG_FEATURES = ['breath_time']\n",
    "    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_in_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_in_time{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_out_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n",
    "    \n",
    "    NOT_WATCH_PARAM = ['INPUT']\n",
    "    \n",
    "    ######################\n",
    "    # Loaders #\n",
    "    ######################\n",
    "    loader_params = {\n",
    "        \"train\": {\n",
    "            'batch_size': 128,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 8,\n",
    "            'pin_memory': True,\n",
    "            'drop_last': True,\n",
    "        },\n",
    "        \"valid\": {\n",
    "            'batch_size': 32,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 8,\n",
    "            'pin_memory': True,\n",
    "            'drop_last': False,\n",
    "        },\n",
    "        \"test\": {\n",
    "            'batch_size': 32,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 8,\n",
    "            'pin_memory': True,\n",
    "            'drop_last': False,\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.set_seed(config.SEED)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(preds, trues, u_outs):\n",
    "    \"\"\"\n",
    "    Metric for the problem, as I understood it.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = trues\n",
    "    w = 1 - u_outs\n",
    "    \n",
    "    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n",
    "    \n",
    "    mae = w * np.abs(y - preds)\n",
    "    mae = mae.sum() / w.sum()\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VentilatorDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, label_dic=None):\n",
    "        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n",
    "        self.label_dic = label_dic\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dfs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        df = self.dfs[item]\n",
    "        X = df[config.ALL_FEATURES].values\n",
    "        y = df['pressure'].values\n",
    "        if self.label_dic is None:\n",
    "            label = [-1]\n",
    "        else:\n",
    "            label = [self.label_dic[i] for i in y]\n",
    "\n",
    "        d = {\n",
    "            \"X\": torch.tensor(X).float(),\n",
    "            \"y\" : torch.tensor(label).long(),\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VentilatorModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VentilatorModel, self).__init__()\n",
    "        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n",
    "        self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n",
    "        self.seq_emb = nn.Sequential(\n",
    "            nn.Linear(12+len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n",
    "            nn.LayerNorm(config.EMBED_SIZE),\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, dropout=0.2, num_layers=4)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n",
    "            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        initrange = 0.1\n",
    "        self.r_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.c_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rc_dot_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rc_sum_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # LSTM\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        # embed\n",
    "        bs = X.shape[0]\n",
    "        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n",
    "        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n",
    "        rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n",
    "        rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n",
    "        \n",
    "        seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n",
    "        emb_x = self.seq_emb(seq_x)\n",
    "        \n",
    "        out, _ = self.lstm(emb_x, None) \n",
    "        logits = self.head(out)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "#     def loss_fn(self, y_pred, y_true):\n",
    "#         loss = nn.CrossEntropyLoss()(y_pred.reshape(-1, 950), y_true.reshape(-1))\n",
    "#         return loss\n",
    "    \n",
    "def loss_fn(y_pred, y_true):\n",
    "#     loss = nn.L1Loss()(y_pred.reshape(-1), y_true.reshape(-1))\n",
    "    loss = nn.CrossEntropyLoss()(y_pred.reshape(-1, 950), y_true.reshape(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, scheduler, loader):\n",
    "    losses, lrs = [], []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for d in loader:\n",
    "        out = model(d['X'].to(device))\n",
    "        loss = loss_fn(out, d['y'].to(device))\n",
    "        losses.append(loss.item())\n",
    "        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n",
    "        lrs.append(step_lr)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    return np.array(losses).mean(), np.array(lrs).mean()\n",
    "\n",
    "\n",
    "def valid_loop(model, loader, target_dic_inv):\n",
    "    losses, predicts = [], []\n",
    "    model.eval()\n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(d['X'].to(device))\n",
    "            loss = loss_fn(out, d['y'].to(device))\n",
    "        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n",
    "        losses.append(loss.item())\n",
    "        predicts.append(out.cpu())\n",
    "\n",
    "    return np.array(losses).mean(), torch.vstack(predicts).numpy().reshape(-1)\n",
    "\n",
    "def test_loop(model, loader, target_dic_inv):\n",
    "    predicts = []\n",
    "    model.eval()\n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(d['X'].to(device))\n",
    "        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n",
    "        predicts.append(out.cpu())\n",
    "\n",
    "    return torch.vstack(predicts).numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature(df):\n",
    "    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "    df['delta'] = df['time_delta'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n",
    "\n",
    "    df['cross']= df['u_in']*df['u_out']\n",
    "    df['cross2']= df['time_step']*df['u_out']\n",
    "    \n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    df['one'] = 1\n",
    "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
    "    df['u_in_cummean'] =df['u_in_cumsum'] / df['count']\n",
    "    \n",
    "    df = df.drop(['count','one'], axis=1)\n",
    "    return df\n",
    "\n",
    "def add_lag_feature(df):\n",
    "    # https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173\n",
    "    for lag in range(1, config.USE_LAG+1):\n",
    "        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n",
    "        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n",
    "\n",
    "        # u_in \n",
    "        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        #df[f'u_in_lag_{lag}_back'] = df['u_in'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n",
    "        #df[f'u_in_time{lag}_back'] = df['u_in'] - df[f'u_in_lag_{lag}_back']\n",
    "        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        #df[f'u_out_lag_{lag}_back'] = df['u_out'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "\n",
    "    # breath_time\n",
    "    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "    df['breath_time'] = df['time_step'] - df['time_step_lag']\n",
    "\n",
    "    drop_columns = ['time_step_lag']\n",
    "    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n",
    "    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    # fill na by zero\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "c_dic = {10: 0, 20: 1, 50:2}\n",
    "r_dic = {5: 0, 20: 1, 50:2}\n",
    "rc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\n",
    "rc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n",
    "\n",
    "def add_category_features(df):\n",
    "    df['C_cate'] = df['C'].map(c_dic)\n",
    "    df['R_cate'] = df['R'].map(r_dic)\n",
    "    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n",
    "    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n",
    "    return df\n",
    "\n",
    "norm_features = config.CONT_FEATURES + config.LAG_FEATURES\n",
    "norm_features = sorted(list(set(config.CONT_FEATURES + config.LAG_FEATURES) - set(['u_out'])), key=norm_features.index)\n",
    "def norm_scale(train_df, test_df):\n",
    "    scaler = RobustScaler()\n",
    "    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n",
    "    scaler.fit(all_u_in)\n",
    "    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n",
    "    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{config.INPUT}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{config.INPUT}/test.csv\")\n",
    "sub_df = pd.read_csv(f\"{config.INPUT}/sample_submission.csv\")\n",
    "\n",
    "\n",
    "# target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n",
    "# target_dic_inv = {v: k for k, v in target_dic.items()}\n",
    "\n",
    "\n",
    "\n",
    "train_df = add_feature(train_df)\n",
    "test_df = add_feature(test_df)\n",
    "train_df = add_lag_feature(train_df)\n",
    "test_df = add_lag_feature(test_df)\n",
    "train_df = add_category_features(train_df)\n",
    "test_df = add_category_features(test_df)\n",
    "train_df, test_df = norm_scale(train_df, test_df)\n",
    "\n",
    "target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n",
    "target_dic_inv = {v: k for k, v in target_dic.items()}\n",
    "\n",
    "test_df['pressure'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-0\n",
      "init LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d47ec48928a45e9abfdeb5bfb07d1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, valid mask score = 1.4688699097629445:\n",
      "epoch = 1, valid mask score = 1.0576114296945414:\n",
      "epoch = 2, valid mask score = 0.7398250125802809:\n",
      "epoch = 3, valid mask score = 0.6860280988317399:\n",
      "epoch = 4, valid mask score = 0.5738272719607976:\n",
      "epoch = 5, valid mask score = 0.5690595201086119:\n",
      "epoch = 6, valid mask score = 0.5230789006986082:\n",
      "epoch = 7, valid mask score = 0.4482845917810134:\n",
      "epoch = 8, valid mask score = 0.5070581710549269:\n",
      "epoch = 9, valid mask score = 0.4392625220856523:\n",
      "epoch = 10, valid mask score = 0.4214289439731365:\n",
      "epoch = 11, valid mask score = 0.44835211986883255:\n",
      "epoch = 12, valid mask score = 0.49979831295807386:\n",
      "epoch = 13, valid mask score = 0.4167444982995409:\n",
      "epoch = 14, valid mask score = 0.3725535635214235:\n",
      "epoch = 15, valid mask score = 0.4401094904555086:\n",
      "epoch = 16, valid mask score = 0.3746929792611828:\n",
      "epoch = 17, valid mask score = 0.35890183225553535:\n",
      "epoch = 18, valid mask score = 0.3542088121423203:\n",
      "epoch = 19, valid mask score = 0.31907814301590665:\n",
      "epoch = 20, valid mask score = 0.31831128902805067:\n",
      "epoch = 21, valid mask score = 0.32329605968698616:\n",
      "epoch = 22, valid mask score = 0.31833783925232034:\n",
      "epoch = 23, valid mask score = 0.2972100234532851:\n",
      "epoch = 24, valid mask score = 0.28764531593960946:\n",
      "epoch = 25, valid mask score = 0.29358493553811355:\n",
      "epoch = 26, valid mask score = 0.2782046090152325:\n",
      "epoch = 27, valid mask score = 0.29157212698589896:\n",
      "epoch = 28, valid mask score = 0.2691357341582316:\n",
      "epoch = 29, valid mask score = 0.27487923153201393:\n",
      "epoch = 30, valid mask score = 0.2912096420630496:\n",
      "epoch = 31, valid mask score = 0.2670285439132554:\n",
      "epoch = 32, valid mask score = 0.24995106693188052:\n",
      "epoch = 33, valid mask score = 0.246885040138088:\n",
      "epoch = 34, valid mask score = 0.23863519757268323:\n",
      "epoch = 35, valid mask score = 0.23874999261442392:\n",
      "epoch = 36, valid mask score = 0.23586796332837284:\n",
      "epoch = 37, valid mask score = 0.2276175089106776:\n",
      "epoch = 38, valid mask score = 0.2254355771218717:\n",
      "epoch = 39, valid mask score = 0.222473600034163:\n",
      "epoch = 40, valid mask score = 0.22002878474197587:\n",
      "epoch = 41, valid mask score = 0.21792619639390115:\n",
      "epoch = 42, valid mask score = 0.21546373069474137:\n",
      "epoch = 43, valid mask score = 0.21443737592849152:\n",
      "epoch = 44, valid mask score = 0.21356555257226542:\n",
      "epoch = 45, valid mask score = 0.21257833056403352:\n",
      "epoch = 46, valid mask score = 0.2120849484855686:\n",
      "epoch = 47, valid mask score = 0.21177802355080366:\n",
      "epoch = 48, valid mask score = 0.21171295481546096:\n",
      "epoch = 49, valid mask score = 0.21167934668369914:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (4823040) does not match length of index (4024000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-be38f9299a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtest_preds_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pressure'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0msub_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{config.OUTPUT}/{config.EXP_NAME}/sub_f{fold}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \"\"\"\n\u001b[1;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \"\"\"\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (4823040) does not match length of index (4024000)"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "test_preds_lst = []\n",
    "\n",
    "gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n",
    "for fold, (_, valid_idx) in enumerate(gkf):\n",
    "    train_df.loc[valid_idx, 'fold'] = fold\n",
    "    \n",
    "for fold in range(config.N_FOLD):\n",
    "    print(f'Fold-{fold}')\n",
    "    \n",
    "    trn_df = train_df.query(f\"fold!={fold}\").reset_index(drop=True)\n",
    "    val_df = train_df.query(f\"fold=={fold}\").reset_index(drop=True)\n",
    "    \n",
    "    loaders = {\n",
    "        phase: DataLoader(\n",
    "            VentilatorDataset(\n",
    "                df_, target_dic\n",
    "            ),\n",
    "            **config.loader_params[phase])  # type: ignore\n",
    "        for phase, df_ in zip([\"train\", \"valid\", \"test\"], [trn_df, val_df, test_df])\n",
    "    }\n",
    "    \n",
    "\n",
    "    model = VentilatorModel()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "    num_train_steps = int(len(loaders['train']) * config.N_EPOCHS)\n",
    "    num_warmup_steps = int(num_train_steps / 10)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "\n",
    "    uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n",
    "#         wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n",
    "#         wandb_config = wandb.config\n",
    "#         wandb_config.fold = fold\n",
    "#         for k, v in dict(vars(config)).items():\n",
    "#             if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n",
    "#                 continue\n",
    "#             wandb_config[k] = v\n",
    "#         wandb.watch(model)\n",
    "\n",
    "    os.makedirs(f'{config.OUTPUT}/{config.EXP_NAME}', exist_ok=True)\n",
    "    model_path = f\"{config.OUTPUT}/{config.EXP_NAME}/ventilator_f{fold}_best_model.bin\"\n",
    "\n",
    "    valid_best_score = float('inf')\n",
    "    valid_best_score_mask = float('inf')\n",
    "    for epoch in tqdm(range(config.N_EPOCHS)):\n",
    "        train_loss, lrs = train_loop(model, optimizer, scheduler, loaders['train'])\n",
    "        valid_loss, valid_predict = valid_loop(model, loaders['valid'], target_dic_inv)\n",
    "       \n",
    "        valid_score_mask = compute_metric(valid_predict, val_df['pressure'].values, val_df['u_out'].values)\n",
    "\n",
    "        print(f\"epoch = {epoch}, valid mask score = {valid_score_mask}:\")\n",
    "\n",
    "        if valid_score_mask < valid_best_score_mask:\n",
    "            valid_best_score_mask = valid_score_mask\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n",
    "\n",
    "#             wandb.log({\n",
    "#                 \"train_loss\": train_loss,\n",
    "#                 \"valid_loss\": valid_loss,\n",
    "#                 \"valid_score\": valid_score,\n",
    "#                 \"valid_best_score\": valid_best_score,\n",
    "#                 \"valid_score_mask\": valid_score_mask,\n",
    "#                 \"valid_best_score_mask\": valid_best_score_mask,\n",
    "#                 \"learning_rate\": lrs,\n",
    "#             })\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test_preds = test_loop(model, loaders['test'], target_dic_inv)\n",
    "    test_preds_lst.append(test_preds)\n",
    "\n",
    "    sub_df['pressure'] = test_preds\n",
    "    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/sub_f{fold}.csv\", index=None)\n",
    "\n",
    "    train_df['oof'] = oof\n",
    "    train_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/oof.csv\", index=None)\n",
    "#         wandb.finish()\n",
    "\n",
    "    del model, optimizer, scheduler, train_loader, valid_loader, train_dset, valid_dset\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n",
    "sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\", index=None)\n",
    "\n",
    "sub_df['pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n",
    "sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\", index=None)\n",
    "\n",
    "# Post Processing: https://www.kaggle.com/snnclsr/a-dummy-approach-to-improve-your-score-postprocess\n",
    "unique_pressures = train_df[\"pressure\"].unique()\n",
    "sorted_pressures = np.sort(unique_pressures)\n",
    "total_pressures_len = len(sorted_pressures)\n",
    "\n",
    "def find_nearest(prediction):\n",
    "    insert_idx = np.searchsorted(sorted_pressures, prediction)\n",
    "    if insert_idx == total_pressures_len:\n",
    "        # If the predicted value is bigger than the highest pressure in the train dataset,\n",
    "        # return the max value.\n",
    "        return sorted_pressures[-1]\n",
    "    elif insert_idx == 0:\n",
    "        # Same control but for the lower bound.\n",
    "        return sorted_pressures[0]\n",
    "    lower_val = sorted_pressures[insert_idx - 1]\n",
    "    upper_val = sorted_pressures[insert_idx]\n",
    "    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n",
    "\n",
    "sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\")\n",
    "sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean_pp.csv\", index=None)\n",
    "\n",
    "sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\")\n",
    "sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median_pp.csv\", index=None)\n",
    "\n",
    "cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n",
    "print(\"CV:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 40, valid mask score = 0.22002878474197587:\n",
    "epoch = 41, valid mask score = 0.21792619639390115:\n",
    "epoch = 42, valid mask score = 0.21546373069474137:\n",
    "epoch = 43, valid mask score = 0.21443737592849152:\n",
    "epoch = 44, valid mask score = 0.21356555257226542:\n",
    "epoch = 45, valid mask score = 0.21257833056403352:\n",
    "epoch = 46, valid mask score = 0.2120849484855686:\n",
    "epoch = 47, valid mask score = 0.21177802355080366:\n",
    "epoch = 48, valid mask score = 0.21171295481546096:\n",
    "epoch = 49, valid mask score = 0.21167934668369914:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

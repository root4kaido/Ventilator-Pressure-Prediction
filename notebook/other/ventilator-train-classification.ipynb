{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sharing code train this dataset as a regression task.\n",
    "\n",
    "But in this code, I train as a classification task.\n",
    "\n",
    "I encode the target value pressure to 950 classes and calculate CrossEntropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-08T06:20:19.786273Z",
     "iopub.status.busy": "2021-10-08T06:20:19.785933Z",
     "iopub.status.idle": "2021-10-08T06:20:19.801265Z",
     "shell.execute_reply": "2021-10-08T06:20:19.800597Z",
     "shell.execute_reply.started": "2021-10-08T06:20:19.786179Z"
    }
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# secret_label = \"wandb\"\n",
    "# secret_value = UserSecretsClient().get_secret(secret_label)\n",
    "# # !wandb login $secret_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "# import wandb\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    EXP_NAME = \"exp068_categorical\"\n",
    "    \n",
    "    INPUT = \"/home/knikaido/work/Ventilator-Pressure-Prediction/data/ventilator-pressure-prediction\"\n",
    "    OUTPUT = \"./output\"\n",
    "    N_FOLD = 5\n",
    "    SEED = 0\n",
    "    \n",
    "    LR = 5e-3\n",
    "    N_EPOCHS = 50\n",
    "    EMBED_SIZE = 64\n",
    "    HIDDEN_SIZE = 256\n",
    "    BS = 512\n",
    "    WEIGHT_DECAY = 1e-3\n",
    "\n",
    "    USE_LAG = 4\n",
    "    CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n",
    "    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2']\n",
    "    LAG_FEATURES = ['breath_time']\n",
    "    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_in_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_in_time{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n",
    "    #LAG_FEATURES += [f'u_out_lag_{i}_back' for i in range(1, USE_LAG+1)]\n",
    "    ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n",
    "    \n",
    "    NOT_WATCH_PARAM = ['INPUT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=config.SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VentilatorDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, label_dic=None):\n",
    "        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n",
    "        self.label_dic = label_dic\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dfs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        df = self.dfs[item]\n",
    "        X = df[config.ALL_FEATURES].values\n",
    "        y = df['pressure'].values\n",
    "        if self.label_dic is None:\n",
    "            label = [-1]\n",
    "        else:\n",
    "            label = [self.label_dic[i] for i in y]\n",
    "\n",
    "        d = {\n",
    "            \"X\": torch.tensor(X).float(),\n",
    "            \"y\" : torch.tensor(y).float(),\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VentilatorModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VentilatorModel, self).__init__()\n",
    "        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n",
    "        self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n",
    "        self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n",
    "        self.seq_emb = nn.Sequential(\n",
    "            nn.Linear(12+len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n",
    "            nn.LayerNorm(config.EMBED_SIZE),\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, dropout=0.2, num_layers=4)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n",
    "            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.HIDDEN_SIZE * 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        initrange = 0.1\n",
    "        self.r_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.c_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rc_dot_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.rc_sum_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # LSTM\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        # embed\n",
    "        bs = X.shape[0]\n",
    "        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n",
    "        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n",
    "        rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n",
    "        rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n",
    "        \n",
    "        seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n",
    "        emb_x = self.seq_emb(seq_x)\n",
    "        \n",
    "        out, _ = self.lstm(emb_x, None) \n",
    "        logits = self.head(out)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = self.loss_fn(logits, y)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "#     def loss_fn(self, y_pred, y_true):\n",
    "#         loss = nn.CrossEntropyLoss()(y_pred.reshape(-1, 950), y_true.reshape(-1))\n",
    "#         return loss\n",
    "    \n",
    "    def loss_fn(self, y_pred, y_true):\n",
    "        loss = nn.L1Loss()(y_pred.reshape(-1), y_true.reshape(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, scheduler, loader):\n",
    "    losses, lrs = [], []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for d in loader:\n",
    "        out, loss = model(d['X'].to(device), d['y'].to(device))\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n",
    "        lrs.append(step_lr)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    return np.array(losses).mean(), np.array(lrs).mean()\n",
    "\n",
    "\n",
    "def valid_loop(model, loader, target_dic_inv):\n",
    "    losses, predicts = [], []\n",
    "    model.eval()\n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            out, loss = model(d['X'].to(device), d['y'].to(device))\n",
    "#         out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n",
    "        losses.append(loss.item())\n",
    "        predicts.append(out.cpu())\n",
    "\n",
    "    return np.array(losses).mean(), torch.vstack(predicts).numpy().reshape(-1)\n",
    "\n",
    "def test_loop(model, loader, target_dic_inv):\n",
    "    predicts = []\n",
    "    model.eval()\n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(d['X'].to(device))\n",
    "#         out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n",
    "        predicts.append(out.cpu())\n",
    "\n",
    "    return torch.vstack(predicts).numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature(df):\n",
    "    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "    df['delta'] = df['time_delta'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n",
    "\n",
    "    df['cross']= df['u_in']*df['u_out']\n",
    "    df['cross2']= df['time_step']*df['u_out']\n",
    "    \n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    df['one'] = 1\n",
    "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
    "    df['u_in_cummean'] =df['u_in_cumsum'] / df['count']\n",
    "    \n",
    "    df = df.drop(['count','one'], axis=1)\n",
    "    return df\n",
    "\n",
    "def add_lag_feature(df):\n",
    "    # https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173\n",
    "    for lag in range(1, config.USE_LAG+1):\n",
    "        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n",
    "        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n",
    "\n",
    "        # u_in \n",
    "        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        #df[f'u_in_lag_{lag}_back'] = df['u_in'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n",
    "        #df[f'u_in_time{lag}_back'] = df['u_in'] - df[f'u_in_lag_{lag}_back']\n",
    "        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "        #df[f'u_out_lag_{lag}_back'] = df['u_out'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "\n",
    "    # breath_time\n",
    "    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n",
    "    df['breath_time'] = df['time_step'] - df['time_step_lag']\n",
    "\n",
    "    drop_columns = ['time_step_lag']\n",
    "    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n",
    "    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    # fill na by zero\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "c_dic = {10: 0, 20: 1, 50:2}\n",
    "r_dic = {5: 0, 20: 1, 50:2}\n",
    "rc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\n",
    "rc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n",
    "\n",
    "def add_category_features(df):\n",
    "    df['C_cate'] = df['C'].map(c_dic)\n",
    "    df['R_cate'] = df['R'].map(r_dic)\n",
    "    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n",
    "    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n",
    "    return df\n",
    "\n",
    "norm_features = config.CONT_FEATURES + config.LAG_FEATURES\n",
    "def norm_scale(train_df, test_df):\n",
    "    scaler = RobustScaler()\n",
    "    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n",
    "    scaler.fit(all_u_in)\n",
    "    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n",
    "    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    train_df = pd.read_csv(f\"{config.INPUT}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{config.INPUT}/test.csv\")\n",
    "    sub_df = pd.read_csv(f\"{config.INPUT}/sample_submission.csv\")\n",
    "    oof = np.zeros(len(train_df))\n",
    "    test_preds_lst = []\n",
    "\n",
    "    target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n",
    "    target_dic_inv = {v: k for k, v in target_dic.items()}\n",
    "\n",
    "    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n",
    "    for fold, (_, valid_idx) in enumerate(gkf):\n",
    "        train_df.loc[valid_idx, 'fold'] = fold\n",
    "\n",
    "    train_df = add_feature(train_df)\n",
    "    test_df = add_feature(test_df)\n",
    "    train_df = add_lag_feature(train_df)\n",
    "    test_df = add_lag_feature(test_df)\n",
    "    train_df = add_category_features(train_df)\n",
    "    test_df = add_category_features(test_df)\n",
    "    train_df, test_df = norm_scale(train_df, test_df)\n",
    "\n",
    "    test_df['pressure'] = -1\n",
    "    test_dset = VentilatorDataset(test_df)\n",
    "    test_loader = DataLoader(test_dset, batch_size=config.BS,\n",
    "                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n",
    "    \n",
    "    for fold in range(config.N_FOLD):\n",
    "        print(f'Fold-{fold}')\n",
    "        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"), target_dic)\n",
    "        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"), target_dic)\n",
    "\n",
    "        set_seed()\n",
    "        train_loader = DataLoader(train_dset, batch_size=config.BS,\n",
    "                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n",
    "                                  worker_init_fn=lambda x: set_seed())\n",
    "        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n",
    "                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n",
    "\n",
    "        model = VentilatorModel()\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "        num_train_steps = int(len(train_loader) * config.N_EPOCHS)\n",
    "        num_warmup_steps = int(num_train_steps / 10)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "\n",
    "        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n",
    "#         wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n",
    "#         wandb_config = wandb.config\n",
    "#         wandb_config.fold = fold\n",
    "#         for k, v in dict(vars(config)).items():\n",
    "#             if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n",
    "#                 continue\n",
    "#             wandb_config[k] = v\n",
    "#         wandb.watch(model)\n",
    "        \n",
    "        os.makedirs(f'{config.OUTPUT}/{config.EXP_NAME}', exist_ok=True)\n",
    "        model_path = f\"{config.OUTPUT}/{config.EXP_NAME}/ventilator_f{fold}_best_model.bin\"\n",
    "        \n",
    "        valid_best_score = float('inf')\n",
    "        valid_best_score_mask = float('inf')\n",
    "        for epoch in tqdm(range(config.N_EPOCHS)):\n",
    "            train_loss, lrs = train_loop(model, optimizer, scheduler, train_loader)\n",
    "            valid_loss, valid_predict = valid_loop(model, valid_loader, target_dic_inv)\n",
    "            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n",
    "\n",
    "            mask = (train_df.query(f\"fold=={fold}\")['u_out'] == 0).values\n",
    "            _score = valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values\n",
    "            valid_score_mask = np.abs(_score[mask]).mean()\n",
    "            print(f\"epoch = {epoch}, valid mask score = {valid_score_mask}:\")\n",
    "\n",
    "            if valid_score < valid_best_score:\n",
    "                valid_best_score = valid_score\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n",
    "\n",
    "            if valid_score_mask < valid_best_score_mask:\n",
    "                valid_best_score_mask = valid_score_mask\n",
    "\n",
    "#             wandb.log({\n",
    "#                 \"train_loss\": train_loss,\n",
    "#                 \"valid_loss\": valid_loss,\n",
    "#                 \"valid_score\": valid_score,\n",
    "#                 \"valid_best_score\": valid_best_score,\n",
    "#                 \"valid_score_mask\": valid_score_mask,\n",
    "#                 \"valid_best_score_mask\": valid_best_score_mask,\n",
    "#                 \"learning_rate\": lrs,\n",
    "#             })\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        test_preds = test_loop(model, test_loader, target_dic_inv)\n",
    "        test_preds_lst.append(test_preds)\n",
    "        \n",
    "        sub_df['pressure'] = test_preds\n",
    "        sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/sub_f{fold}.csv\", index=None)\n",
    "\n",
    "        train_df['oof'] = oof\n",
    "        train_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/oof.csv\", index=None)\n",
    "#         wandb.finish()\n",
    "\n",
    "        del model, optimizer, scheduler, train_loader, valid_loader, train_dset, valid_dset\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n",
    "    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\", index=None)\n",
    "\n",
    "    sub_df['pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n",
    "    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\", index=None)\n",
    "    \n",
    "    # Post Processing: https://www.kaggle.com/snnclsr/a-dummy-approach-to-improve-your-score-postprocess\n",
    "    unique_pressures = train_df[\"pressure\"].unique()\n",
    "    sorted_pressures = np.sort(unique_pressures)\n",
    "    total_pressures_len = len(sorted_pressures)\n",
    "\n",
    "    def find_nearest(prediction):\n",
    "        insert_idx = np.searchsorted(sorted_pressures, prediction)\n",
    "        if insert_idx == total_pressures_len:\n",
    "            # If the predicted value is bigger than the highest pressure in the train dataset,\n",
    "            # return the max value.\n",
    "            return sorted_pressures[-1]\n",
    "        elif insert_idx == 0:\n",
    "            # Same control but for the lower bound.\n",
    "            return sorted_pressures[0]\n",
    "        lower_val = sorted_pressures[insert_idx - 1]\n",
    "        upper_val = sorted_pressures[insert_idx]\n",
    "        return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n",
    "    \n",
    "    sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\")\n",
    "    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean_pp.csv\", index=None)\n",
    "    \n",
    "    sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\")\n",
    "    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median_pp.csv\", index=None)\n",
    "    \n",
    "    cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n",
    "    print(\"CV:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-0\n",
      "init LSTM(64, 256, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfc0d25c44046fbbb4f0ab58880a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, valid mask score = 0.9453852457629062:\n",
      "epoch = 1, valid mask score = 0.5194634257366699:\n",
      "epoch = 2, valid mask score = 0.39860576686053933:\n",
      "epoch = 3, valid mask score = 0.3416326869521652:\n",
      "epoch = 4, valid mask score = 0.31051688770593844:\n",
      "epoch = 5, valid mask score = 0.2859712235129969:\n",
      "epoch = 6, valid mask score = 0.265067004639667:\n",
      "epoch = 7, valid mask score = 0.26538234830239704:\n",
      "epoch = 8, valid mask score = 0.2774170608882531:\n",
      "epoch = 9, valid mask score = 0.23979488175541994:\n",
      "epoch = 10, valid mask score = 0.22913753610826065:\n",
      "epoch = 11, valid mask score = 0.2335857793744477:\n",
      "epoch = 12, valid mask score = 0.24218560024604402:\n",
      "epoch = 13, valid mask score = 0.22183550076256728:\n",
      "epoch = 14, valid mask score = 0.22219287013361985:\n",
      "epoch = 15, valid mask score = 0.21858413479405833:\n",
      "epoch = 16, valid mask score = 0.2276371927379565:\n",
      "epoch = 17, valid mask score = 0.21236664509831268:\n",
      "epoch = 18, valid mask score = 0.2007667542788717:\n",
      "epoch = 19, valid mask score = 0.2140583990094257:\n",
      "epoch = 20, valid mask score = 0.1976497396746342:\n",
      "epoch = 21, valid mask score = 0.19976886539374572:\n",
      "epoch = 22, valid mask score = 0.1934106317211466:\n",
      "epoch = 23, valid mask score = 0.21062259132541208:\n",
      "epoch = 24, valid mask score = 0.20633847825850635:\n",
      "epoch = 25, valid mask score = 0.21711858262087866:\n",
      "epoch = 26, valid mask score = 0.19669376476464626:\n",
      "epoch = 27, valid mask score = 0.1862902572130549:\n",
      "epoch = 28, valid mask score = 0.1839666148099082:\n",
      "epoch = 29, valid mask score = 0.18416456131571168:\n",
      "epoch = 30, valid mask score = 0.18464050498038037:\n",
      "epoch = 31, valid mask score = 0.1857880001005783:\n",
      "epoch = 32, valid mask score = 0.1840972038825883:\n",
      "epoch = 33, valid mask score = 0.18104881096987926:\n",
      "epoch = 34, valid mask score = 0.17908620660233518:\n",
      "epoch = 35, valid mask score = 0.17613202765447614:\n",
      "epoch = 36, valid mask score = 0.17504571823690793:\n",
      "epoch = 37, valid mask score = 0.17294233451032484:\n",
      "epoch = 38, valid mask score = 0.17158734102058942:\n",
      "epoch = 39, valid mask score = 0.1714068504247864:\n",
      "epoch = 40, valid mask score = 0.1698339211748706:\n",
      "epoch = 41, valid mask score = 0.16995628319715436:\n",
      "epoch = 42, valid mask score = 0.169247209983279:\n",
      "epoch = 43, valid mask score = 0.16827184677875487:\n",
      "epoch = 44, valid mask score = 0.16765128415018035:\n",
      "epoch = 45, valid mask score = 0.16757432014460813:\n",
      "epoch = 46, valid mask score = 0.16716097448129638:\n",
      "epoch = 47, valid mask score = 0.167125166072959:\n",
      "epoch = 48, valid mask score = 0.1669732000361485:\n",
      "epoch = 49, valid mask score = 0.1669375040177583:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-5c3fe201a37d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oof'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{config.OUTPUT}/{config.EXP_NAME}/oof.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;31m#         wandb.finish()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m             )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

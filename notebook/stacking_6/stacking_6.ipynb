{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    exp_num = 6\n",
    "    n_folds = 10\n",
    "    folds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    seed = 777\n",
    "    local = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/home/knikaido/work/Ventilator-Pressure-Prediction/data/ventilator-pressure-prediction\")\n",
    "OUTPUT_DIR = Path('./output/')\n",
    "OOF_DIR = Path(\"/home/knikaido/work/Ventilator-Pressure-Prediction/data/team_oofs_stacking\")\n",
    "SUB_DIR = Path(\"/home/knikaido/work/Ventilator-Pressure-Prediction/data/team_subs_stacking\")\n",
    "PICKLE_DIR = Path(\"/home/knikaido/work/Ventilator-Pressure-Prediction/data/team_stacking_pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../src/')\n",
    "import utils as utils\n",
    "from utils import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PICKLE_DIR / \"oof_ver1102_cv0.1166.npy\", mode=\"rb\") as f:\n",
    "    oofs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.843223</td>\n",
       "      <td>6.287325</td>\n",
       "      <td>5.828472</td>\n",
       "      <td>5.783465</td>\n",
       "      <td>6.013763</td>\n",
       "      <td>6.251595</td>\n",
       "      <td>5.823515</td>\n",
       "      <td>5.843684</td>\n",
       "      <td>7.629176</td>\n",
       "      <td>5.816144</td>\n",
       "      <td>6.452471</td>\n",
       "      <td>5.834950</td>\n",
       "      <td>6.206788</td>\n",
       "      <td>6.402203</td>\n",
       "      <td>5.788936</td>\n",
       "      <td>5.816258</td>\n",
       "      <td>6.137226</td>\n",
       "      <td>5.962987</td>\n",
       "      <td>6.189220</td>\n",
       "      <td>6.888747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.873457</td>\n",
       "      <td>5.989612</td>\n",
       "      <td>5.880560</td>\n",
       "      <td>5.869486</td>\n",
       "      <td>6.103593</td>\n",
       "      <td>5.920428</td>\n",
       "      <td>5.898210</td>\n",
       "      <td>5.854125</td>\n",
       "      <td>6.086765</td>\n",
       "      <td>5.794350</td>\n",
       "      <td>5.884715</td>\n",
       "      <td>5.841093</td>\n",
       "      <td>5.789935</td>\n",
       "      <td>5.939039</td>\n",
       "      <td>5.865545</td>\n",
       "      <td>5.818817</td>\n",
       "      <td>5.971342</td>\n",
       "      <td>5.830175</td>\n",
       "      <td>5.965767</td>\n",
       "      <td>6.035792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.892688</td>\n",
       "      <td>8.725241</td>\n",
       "      <td>8.157262</td>\n",
       "      <td>7.865620</td>\n",
       "      <td>8.852818</td>\n",
       "      <td>8.440607</td>\n",
       "      <td>7.930587</td>\n",
       "      <td>7.914377</td>\n",
       "      <td>8.957220</td>\n",
       "      <td>7.781632</td>\n",
       "      <td>8.618551</td>\n",
       "      <td>7.889086</td>\n",
       "      <td>8.631878</td>\n",
       "      <td>8.335553</td>\n",
       "      <td>7.870878</td>\n",
       "      <td>7.932899</td>\n",
       "      <td>8.829952</td>\n",
       "      <td>8.317939</td>\n",
       "      <td>8.886585</td>\n",
       "      <td>8.744663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.861191</td>\n",
       "      <td>12.752718</td>\n",
       "      <td>12.129442</td>\n",
       "      <td>11.779996</td>\n",
       "      <td>12.312957</td>\n",
       "      <td>12.190743</td>\n",
       "      <td>12.042971</td>\n",
       "      <td>11.954165</td>\n",
       "      <td>12.609935</td>\n",
       "      <td>11.814193</td>\n",
       "      <td>12.376060</td>\n",
       "      <td>11.697797</td>\n",
       "      <td>12.434288</td>\n",
       "      <td>12.549238</td>\n",
       "      <td>11.836457</td>\n",
       "      <td>11.926755</td>\n",
       "      <td>12.402763</td>\n",
       "      <td>12.191305</td>\n",
       "      <td>12.584689</td>\n",
       "      <td>12.658657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.331974</td>\n",
       "      <td>12.974612</td>\n",
       "      <td>12.576923</td>\n",
       "      <td>12.318006</td>\n",
       "      <td>12.656638</td>\n",
       "      <td>12.622059</td>\n",
       "      <td>12.474959</td>\n",
       "      <td>12.393296</td>\n",
       "      <td>12.761421</td>\n",
       "      <td>12.382209</td>\n",
       "      <td>12.714886</td>\n",
       "      <td>12.329643</td>\n",
       "      <td>12.796815</td>\n",
       "      <td>12.954475</td>\n",
       "      <td>12.394281</td>\n",
       "      <td>12.622350</td>\n",
       "      <td>12.992664</td>\n",
       "      <td>12.782994</td>\n",
       "      <td>12.491414</td>\n",
       "      <td>13.061924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290963</th>\n",
       "      <td>29.492047</td>\n",
       "      <td>29.437429</td>\n",
       "      <td>29.562796</td>\n",
       "      <td>29.377239</td>\n",
       "      <td>29.468651</td>\n",
       "      <td>29.447411</td>\n",
       "      <td>29.416391</td>\n",
       "      <td>29.452370</td>\n",
       "      <td>29.423307</td>\n",
       "      <td>29.502108</td>\n",
       "      <td>29.428267</td>\n",
       "      <td>29.397783</td>\n",
       "      <td>29.464108</td>\n",
       "      <td>29.489416</td>\n",
       "      <td>29.432344</td>\n",
       "      <td>29.445993</td>\n",
       "      <td>29.318501</td>\n",
       "      <td>29.588949</td>\n",
       "      <td>29.397257</td>\n",
       "      <td>29.470612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290964</th>\n",
       "      <td>29.098768</td>\n",
       "      <td>29.094450</td>\n",
       "      <td>29.233667</td>\n",
       "      <td>29.051178</td>\n",
       "      <td>29.123398</td>\n",
       "      <td>29.121632</td>\n",
       "      <td>29.090053</td>\n",
       "      <td>29.107479</td>\n",
       "      <td>29.083748</td>\n",
       "      <td>29.150092</td>\n",
       "      <td>29.077028</td>\n",
       "      <td>29.089489</td>\n",
       "      <td>29.140293</td>\n",
       "      <td>29.162943</td>\n",
       "      <td>29.157043</td>\n",
       "      <td>29.128893</td>\n",
       "      <td>29.051003</td>\n",
       "      <td>29.192144</td>\n",
       "      <td>29.049335</td>\n",
       "      <td>29.123194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290965</th>\n",
       "      <td>29.777598</td>\n",
       "      <td>29.816580</td>\n",
       "      <td>29.977200</td>\n",
       "      <td>29.847507</td>\n",
       "      <td>29.894068</td>\n",
       "      <td>29.917118</td>\n",
       "      <td>29.855816</td>\n",
       "      <td>29.861362</td>\n",
       "      <td>29.875168</td>\n",
       "      <td>29.872925</td>\n",
       "      <td>29.759693</td>\n",
       "      <td>29.833424</td>\n",
       "      <td>29.884844</td>\n",
       "      <td>29.935467</td>\n",
       "      <td>29.868884</td>\n",
       "      <td>29.900164</td>\n",
       "      <td>29.707857</td>\n",
       "      <td>29.999273</td>\n",
       "      <td>29.811031</td>\n",
       "      <td>29.882458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290966</th>\n",
       "      <td>29.344897</td>\n",
       "      <td>29.375248</td>\n",
       "      <td>29.443771</td>\n",
       "      <td>29.373464</td>\n",
       "      <td>29.389355</td>\n",
       "      <td>29.424540</td>\n",
       "      <td>29.361870</td>\n",
       "      <td>29.397663</td>\n",
       "      <td>29.361753</td>\n",
       "      <td>29.391775</td>\n",
       "      <td>29.331190</td>\n",
       "      <td>29.346144</td>\n",
       "      <td>29.458084</td>\n",
       "      <td>29.466442</td>\n",
       "      <td>29.400556</td>\n",
       "      <td>29.366915</td>\n",
       "      <td>29.330189</td>\n",
       "      <td>29.421896</td>\n",
       "      <td>29.354986</td>\n",
       "      <td>29.395554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290967</th>\n",
       "      <td>29.955917</td>\n",
       "      <td>30.020460</td>\n",
       "      <td>30.144281</td>\n",
       "      <td>30.003941</td>\n",
       "      <td>30.114656</td>\n",
       "      <td>30.106968</td>\n",
       "      <td>30.010452</td>\n",
       "      <td>30.005215</td>\n",
       "      <td>30.125376</td>\n",
       "      <td>30.040400</td>\n",
       "      <td>30.000271</td>\n",
       "      <td>30.023182</td>\n",
       "      <td>30.097168</td>\n",
       "      <td>30.105059</td>\n",
       "      <td>30.068122</td>\n",
       "      <td>30.049168</td>\n",
       "      <td>30.028421</td>\n",
       "      <td>30.181812</td>\n",
       "      <td>30.089697</td>\n",
       "      <td>30.050657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2290968 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred_0     pred_1     pred_2     pred_3     pred_4     pred_5  \\\n",
       "0         5.843223   6.287325   5.828472   5.783465   6.013763   6.251595   \n",
       "1         5.873457   5.989612   5.880560   5.869486   6.103593   5.920428   \n",
       "2         7.892688   8.725241   8.157262   7.865620   8.852818   8.440607   \n",
       "3        11.861191  12.752718  12.129442  11.779996  12.312957  12.190743   \n",
       "4        12.331974  12.974612  12.576923  12.318006  12.656638  12.622059   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "2290963  29.492047  29.437429  29.562796  29.377239  29.468651  29.447411   \n",
       "2290964  29.098768  29.094450  29.233667  29.051178  29.123398  29.121632   \n",
       "2290965  29.777598  29.816580  29.977200  29.847507  29.894068  29.917118   \n",
       "2290966  29.344897  29.375248  29.443771  29.373464  29.389355  29.424540   \n",
       "2290967  29.955917  30.020460  30.144281  30.003941  30.114656  30.106968   \n",
       "\n",
       "            pred_6     pred_7     pred_8     pred_9    pred_10    pred_11  \\\n",
       "0         5.823515   5.843684   7.629176   5.816144   6.452471   5.834950   \n",
       "1         5.898210   5.854125   6.086765   5.794350   5.884715   5.841093   \n",
       "2         7.930587   7.914377   8.957220   7.781632   8.618551   7.889086   \n",
       "3        12.042971  11.954165  12.609935  11.814193  12.376060  11.697797   \n",
       "4        12.474959  12.393296  12.761421  12.382209  12.714886  12.329643   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "2290963  29.416391  29.452370  29.423307  29.502108  29.428267  29.397783   \n",
       "2290964  29.090053  29.107479  29.083748  29.150092  29.077028  29.089489   \n",
       "2290965  29.855816  29.861362  29.875168  29.872925  29.759693  29.833424   \n",
       "2290966  29.361870  29.397663  29.361753  29.391775  29.331190  29.346144   \n",
       "2290967  30.010452  30.005215  30.125376  30.040400  30.000271  30.023182   \n",
       "\n",
       "           pred_12    pred_13    pred_14    pred_15    pred_16    pred_17  \\\n",
       "0         6.206788   6.402203   5.788936   5.816258   6.137226   5.962987   \n",
       "1         5.789935   5.939039   5.865545   5.818817   5.971342   5.830175   \n",
       "2         8.631878   8.335553   7.870878   7.932899   8.829952   8.317939   \n",
       "3        12.434288  12.549238  11.836457  11.926755  12.402763  12.191305   \n",
       "4        12.796815  12.954475  12.394281  12.622350  12.992664  12.782994   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "2290963  29.464108  29.489416  29.432344  29.445993  29.318501  29.588949   \n",
       "2290964  29.140293  29.162943  29.157043  29.128893  29.051003  29.192144   \n",
       "2290965  29.884844  29.935467  29.868884  29.900164  29.707857  29.999273   \n",
       "2290966  29.458084  29.466442  29.400556  29.366915  29.330189  29.421896   \n",
       "2290967  30.097168  30.105059  30.068122  30.049168  30.028421  30.181812   \n",
       "\n",
       "           pred_18    pred_19  \n",
       "0         6.189220   6.888747  \n",
       "1         5.965767   6.035792  \n",
       "2         8.886585   8.744663  \n",
       "3        12.584689  12.658657  \n",
       "4        12.491414  13.061924  \n",
       "...            ...        ...  \n",
       "2290963  29.397257  29.470612  \n",
       "2290964  29.049335  29.123194  \n",
       "2290965  29.811031  29.882458  \n",
       "2290966  29.354986  29.395554  \n",
       "2290967  30.089697  30.050657  \n",
       "\n",
       "[2290968 rows x 20 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oofs = pd.DataFrame(oofs)\n",
    "oofs.columns = [f\"pred_{i}\" for i in range(len(oofs.columns))]\n",
    "oofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PICKLE_DIR / \"pred_ver1102_cv0.1166.npy\", mode=\"rb\") as f:\n",
    "    subs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.237001</td>\n",
       "      <td>6.190098</td>\n",
       "      <td>6.258121</td>\n",
       "      <td>6.315998</td>\n",
       "      <td>6.157633</td>\n",
       "      <td>6.153747</td>\n",
       "      <td>6.301814</td>\n",
       "      <td>6.295876</td>\n",
       "      <td>6.113052</td>\n",
       "      <td>6.245053</td>\n",
       "      <td>6.208011</td>\n",
       "      <td>6.291997</td>\n",
       "      <td>6.191348</td>\n",
       "      <td>5.868285</td>\n",
       "      <td>6.307113</td>\n",
       "      <td>6.306786</td>\n",
       "      <td>6.112450</td>\n",
       "      <td>6.245206</td>\n",
       "      <td>5.842953</td>\n",
       "      <td>6.097662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.938651</td>\n",
       "      <td>6.075787</td>\n",
       "      <td>5.953184</td>\n",
       "      <td>5.938564</td>\n",
       "      <td>6.029568</td>\n",
       "      <td>6.061356</td>\n",
       "      <td>5.943007</td>\n",
       "      <td>5.965106</td>\n",
       "      <td>5.969356</td>\n",
       "      <td>5.935810</td>\n",
       "      <td>6.033892</td>\n",
       "      <td>5.990556</td>\n",
       "      <td>6.055338</td>\n",
       "      <td>5.722659</td>\n",
       "      <td>5.943791</td>\n",
       "      <td>5.968692</td>\n",
       "      <td>5.998645</td>\n",
       "      <td>5.940457</td>\n",
       "      <td>4.952855</td>\n",
       "      <td>6.024591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.218726</td>\n",
       "      <td>7.236039</td>\n",
       "      <td>7.180656</td>\n",
       "      <td>7.115508</td>\n",
       "      <td>7.136393</td>\n",
       "      <td>7.312749</td>\n",
       "      <td>7.136607</td>\n",
       "      <td>7.117581</td>\n",
       "      <td>7.113848</td>\n",
       "      <td>7.179698</td>\n",
       "      <td>7.160792</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>7.202180</td>\n",
       "      <td>6.966420</td>\n",
       "      <td>7.128859</td>\n",
       "      <td>7.112571</td>\n",
       "      <td>7.203706</td>\n",
       "      <td>7.243257</td>\n",
       "      <td>6.766903</td>\n",
       "      <td>7.093171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.752637</td>\n",
       "      <td>7.880373</td>\n",
       "      <td>7.789594</td>\n",
       "      <td>7.607679</td>\n",
       "      <td>7.800501</td>\n",
       "      <td>7.841540</td>\n",
       "      <td>7.651355</td>\n",
       "      <td>7.631863</td>\n",
       "      <td>7.706476</td>\n",
       "      <td>7.734634</td>\n",
       "      <td>7.800667</td>\n",
       "      <td>7.654799</td>\n",
       "      <td>7.776278</td>\n",
       "      <td>7.603629</td>\n",
       "      <td>7.615316</td>\n",
       "      <td>7.638148</td>\n",
       "      <td>7.784928</td>\n",
       "      <td>7.787680</td>\n",
       "      <td>7.563944</td>\n",
       "      <td>7.729118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.262370</td>\n",
       "      <td>9.373093</td>\n",
       "      <td>9.275614</td>\n",
       "      <td>9.145709</td>\n",
       "      <td>9.164462</td>\n",
       "      <td>9.336922</td>\n",
       "      <td>9.160076</td>\n",
       "      <td>9.132385</td>\n",
       "      <td>9.256947</td>\n",
       "      <td>9.260639</td>\n",
       "      <td>9.270784</td>\n",
       "      <td>9.161454</td>\n",
       "      <td>9.304310</td>\n",
       "      <td>9.119343</td>\n",
       "      <td>9.128102</td>\n",
       "      <td>9.155414</td>\n",
       "      <td>9.297371</td>\n",
       "      <td>9.244328</td>\n",
       "      <td>8.998842</td>\n",
       "      <td>9.261788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023995</th>\n",
       "      <td>6.187890</td>\n",
       "      <td>14.774168</td>\n",
       "      <td>35.090607</td>\n",
       "      <td>15.863170</td>\n",
       "      <td>18.102705</td>\n",
       "      <td>16.602873</td>\n",
       "      <td>18.055397</td>\n",
       "      <td>6.150437</td>\n",
       "      <td>12.287502</td>\n",
       "      <td>6.208994</td>\n",
       "      <td>14.476089</td>\n",
       "      <td>6.163520</td>\n",
       "      <td>16.766878</td>\n",
       "      <td>15.343576</td>\n",
       "      <td>15.152164</td>\n",
       "      <td>6.201168</td>\n",
       "      <td>11.471126</td>\n",
       "      <td>37.098095</td>\n",
       "      <td>12.943045</td>\n",
       "      <td>13.821736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023996</th>\n",
       "      <td>6.290313</td>\n",
       "      <td>14.924815</td>\n",
       "      <td>35.653534</td>\n",
       "      <td>16.148630</td>\n",
       "      <td>18.367050</td>\n",
       "      <td>17.279026</td>\n",
       "      <td>20.852467</td>\n",
       "      <td>6.322860</td>\n",
       "      <td>12.271587</td>\n",
       "      <td>6.269710</td>\n",
       "      <td>14.706350</td>\n",
       "      <td>6.320943</td>\n",
       "      <td>16.987122</td>\n",
       "      <td>14.469177</td>\n",
       "      <td>15.314502</td>\n",
       "      <td>6.285121</td>\n",
       "      <td>11.820371</td>\n",
       "      <td>39.119705</td>\n",
       "      <td>12.721097</td>\n",
       "      <td>13.610198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023997</th>\n",
       "      <td>6.397727</td>\n",
       "      <td>14.827735</td>\n",
       "      <td>36.066963</td>\n",
       "      <td>16.197741</td>\n",
       "      <td>17.861053</td>\n",
       "      <td>17.451265</td>\n",
       "      <td>22.164679</td>\n",
       "      <td>6.358964</td>\n",
       "      <td>12.555499</td>\n",
       "      <td>6.435824</td>\n",
       "      <td>14.665152</td>\n",
       "      <td>6.379314</td>\n",
       "      <td>16.780479</td>\n",
       "      <td>15.494080</td>\n",
       "      <td>15.419031</td>\n",
       "      <td>6.437337</td>\n",
       "      <td>11.176121</td>\n",
       "      <td>39.369545</td>\n",
       "      <td>12.846916</td>\n",
       "      <td>14.453132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023998</th>\n",
       "      <td>6.175601</td>\n",
       "      <td>16.304645</td>\n",
       "      <td>34.567181</td>\n",
       "      <td>16.050034</td>\n",
       "      <td>16.602900</td>\n",
       "      <td>17.901031</td>\n",
       "      <td>23.106366</td>\n",
       "      <td>6.156576</td>\n",
       "      <td>13.272074</td>\n",
       "      <td>6.193621</td>\n",
       "      <td>15.665402</td>\n",
       "      <td>6.157371</td>\n",
       "      <td>17.971500</td>\n",
       "      <td>16.013962</td>\n",
       "      <td>15.357834</td>\n",
       "      <td>6.179539</td>\n",
       "      <td>13.108645</td>\n",
       "      <td>37.976372</td>\n",
       "      <td>13.174618</td>\n",
       "      <td>14.309614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023999</th>\n",
       "      <td>6.325679</td>\n",
       "      <td>18.336931</td>\n",
       "      <td>20.707859</td>\n",
       "      <td>16.908670</td>\n",
       "      <td>16.959599</td>\n",
       "      <td>19.662155</td>\n",
       "      <td>27.095233</td>\n",
       "      <td>6.373726</td>\n",
       "      <td>14.272245</td>\n",
       "      <td>6.298654</td>\n",
       "      <td>16.909633</td>\n",
       "      <td>6.370057</td>\n",
       "      <td>19.561602</td>\n",
       "      <td>16.631899</td>\n",
       "      <td>15.934608</td>\n",
       "      <td>6.358223</td>\n",
       "      <td>13.798697</td>\n",
       "      <td>23.645134</td>\n",
       "      <td>16.760270</td>\n",
       "      <td>14.974597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4024000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred_0     pred_1     pred_2     pred_3     pred_4     pred_5  \\\n",
       "0        6.237001   6.190098   6.258121   6.315998   6.157633   6.153747   \n",
       "1        5.938651   6.075787   5.953184   5.938564   6.029568   6.061356   \n",
       "2        7.218726   7.236039   7.180656   7.115508   7.136393   7.312749   \n",
       "3        7.752637   7.880373   7.789594   7.607679   7.800501   7.841540   \n",
       "4        9.262370   9.373093   9.275614   9.145709   9.164462   9.336922   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "4023995  6.187890  14.774168  35.090607  15.863170  18.102705  16.602873   \n",
       "4023996  6.290313  14.924815  35.653534  16.148630  18.367050  17.279026   \n",
       "4023997  6.397727  14.827735  36.066963  16.197741  17.861053  17.451265   \n",
       "4023998  6.175601  16.304645  34.567181  16.050034  16.602900  17.901031   \n",
       "4023999  6.325679  18.336931  20.707859  16.908670  16.959599  19.662155   \n",
       "\n",
       "            pred_6    pred_7     pred_8    pred_9    pred_10   pred_11  \\\n",
       "0         6.301814  6.295876   6.113052  6.245053   6.208011  6.291997   \n",
       "1         5.943007  5.965106   5.969356  5.935810   6.033892  5.990556   \n",
       "2         7.136607  7.117581   7.113848  7.179698   7.160792  7.142857   \n",
       "3         7.651355  7.631863   7.706476  7.734634   7.800667  7.654799   \n",
       "4         9.160076  9.132385   9.256947  9.260639   9.270784  9.161454   \n",
       "...            ...       ...        ...       ...        ...       ...   \n",
       "4023995  18.055397  6.150437  12.287502  6.208994  14.476089  6.163520   \n",
       "4023996  20.852467  6.322860  12.271587  6.269710  14.706350  6.320943   \n",
       "4023997  22.164679  6.358964  12.555499  6.435824  14.665152  6.379314   \n",
       "4023998  23.106366  6.156576  13.272074  6.193621  15.665402  6.157371   \n",
       "4023999  27.095233  6.373726  14.272245  6.298654  16.909633  6.370057   \n",
       "\n",
       "           pred_12    pred_13    pred_14   pred_15    pred_16    pred_17  \\\n",
       "0         6.191348   5.868285   6.307113  6.306786   6.112450   6.245206   \n",
       "1         6.055338   5.722659   5.943791  5.968692   5.998645   5.940457   \n",
       "2         7.202180   6.966420   7.128859  7.112571   7.203706   7.243257   \n",
       "3         7.776278   7.603629   7.615316  7.638148   7.784928   7.787680   \n",
       "4         9.304310   9.119343   9.128102  9.155414   9.297371   9.244328   \n",
       "...            ...        ...        ...       ...        ...        ...   \n",
       "4023995  16.766878  15.343576  15.152164  6.201168  11.471126  37.098095   \n",
       "4023996  16.987122  14.469177  15.314502  6.285121  11.820371  39.119705   \n",
       "4023997  16.780479  15.494080  15.419031  6.437337  11.176121  39.369545   \n",
       "4023998  17.971500  16.013962  15.357834  6.179539  13.108645  37.976372   \n",
       "4023999  19.561602  16.631899  15.934608  6.358223  13.798697  23.645134   \n",
       "\n",
       "           pred_18    pred_19  \n",
       "0         5.842953   6.097662  \n",
       "1         4.952855   6.024591  \n",
       "2         6.766903   7.093171  \n",
       "3         7.563944   7.729118  \n",
       "4         8.998842   9.261788  \n",
       "...            ...        ...  \n",
       "4023995  12.943045  13.821736  \n",
       "4023996  12.721097  13.610198  \n",
       "4023997  12.846916  14.453132  \n",
       "4023998  13.174618  14.309614  \n",
       "4023999  16.760270  14.974597  \n",
       "\n",
       "[4024000 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs = pd.DataFrame(subs)\n",
    "subs.columns = [f\"pred_{i}\" for i in range(len(subs.columns))]\n",
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_features(input_df, dataType = 'train'):\n",
    "    colum = ['time_step', 'u_in', 'R', 'C']\n",
    "\n",
    "    return input_df[colum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_features(input_df, dataType = 'train'):\n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    colum = ['R_C']\n",
    "    rc_map = {'5_10': 0, '5_20': 1, '5_50': 2, '20_10': 3, '20_20': 4, '20_50': 5, '50_10': 6, '50_20': 7, '50_50': 8}\n",
    "    \n",
    "    output_df['R_C'] = [f'{r}_{c}' for r, c in zip(output_df['R'], output_df['C'])]\n",
    "    output_df['R_C'] = output_df['R_C'].map(rc_map)\n",
    "\n",
    "    return output_df[colum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_calc_features(input_df, dataType = 'train'):\n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    output_df['time_delta'] = output_df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
    "    output_df['delta'] = output_df['time_delta'] * output_df['u_in']\n",
    "    output_df['area'] = output_df.groupby('breath_id')['delta'].cumsum()\n",
    "\n",
    "    output_df['cross']= output_df['u_in']*output_df['u_out']\n",
    "    output_df['cross2']= output_df['time_step']*output_df['u_out']\n",
    "    \n",
    "    output_df['u_in_cumsum'] = (output_df['u_in']).groupby(output_df['breath_id']).cumsum()\n",
    "    output_df['one'] = 1\n",
    "    output_df['count'] = (output_df['one']).groupby(output_df['breath_id']).cumsum()\n",
    "    output_df['u_in_cummean'] =output_df['u_in_cumsum'] / output_df['count']\n",
    "    \n",
    "    output_df['u_in_sqrt'] = output_df['u_in'].apply(lambda x: np.sqrt(x))\n",
    "    output_df['u_in_sqrt_cumsum'] = output_df.groupby('breath_id')['u_in_sqrt'].cumsum()\n",
    "    \n",
    "    output_df = output_df.drop(['count','one'], axis=1)\n",
    "    \n",
    "    return output_df.iloc[:, c_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'u_in': [np.max, np.mean],\n",
    "    }\n",
    "    \n",
    "    def get_agg_window(start_time=0, end_time=3.0, add_suffix = False):\n",
    "        \n",
    "        df_tgt = output_df[(output_df['time_step'] >= start_time) & (output_df['time_step'] <= end_time)]\n",
    "        df_feature = df_tgt.groupby(['breath_id']).agg(create_feature_dict)\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(start_time) + '_' + str(end_time))\n",
    "            \n",
    "        return df_feature\n",
    "    \n",
    "    df_agg_feature = get_agg_window().reset_index()\n",
    "    \n",
    "#     df_tmp = get_agg_window(start_time = 2, add_suffix = True).reset_index()\n",
    "#     df_agg_feature = df_agg_feature.merge(df_tmp, how = 'left', on = 'breath_id')\n",
    "#     df_tmp = get_agg_window(start_time = 1, add_suffix = True).reset_index()\n",
    "#     df_agg_feature = df_agg_feature.merge(df_tmp, how = 'left', on = 'breath_id')\n",
    "#     df_tmp = get_agg_window(end_time = 1, add_suffix = True).reset_index()\n",
    "#     df_agg_feature = df_agg_feature.merge(df_tmp, how = 'left', on = 'breath_id')\n",
    "#     df_tmp = get_agg_window(end_time = 2, add_suffix = True).reset_index()\n",
    "#     df_agg_feature = df_agg_feature.merge(df_tmp, how = 'left', on = 'breath_id')\n",
    "\n",
    "    output_df = pd.merge(output_df, df_agg_feature, how='left', on='breath_id')\n",
    "    \n",
    "    output_df['u_in_diffmax'] = output_df['u_in_amax'] - output_df['u_in']\n",
    "    output_df['u_in_diffmean'] = output_df['u_in_mean'] - output_df['u_in']\n",
    "    \n",
    "#     output_df = output_df.drop(['u_in_amax','u_in_mean'], axis=1)\n",
    "    \n",
    "    return output_df.iloc[:, c_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_half_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    output_df['tmp'] = output_df['u_out']*(-1)+1 # inversion of u_out\n",
    "    output_df['u_in_half'] = output_df['tmp'] * output_df['u_in']\n",
    "    \n",
    "#     u_in_half_max_dict = train.groupby('breath_id')['u_in_half'].max().to_dict()\n",
    "#     train['u_in_half_max'] = train['breath_id'].map(u_in_half_max_dict)\n",
    "#     u_in_half_min_dict = train.groupby('breath_id')['u_in_half'].min().to_dict()\n",
    "#     train['u_in_half_min'] = train['breath_id'].map(u_in_half_min_dict)\n",
    "    u_in_half_mean_dict = output_df.groupby('breath_id')['u_in_half'].mean().to_dict()\n",
    "    output_df['u_in_half_mean'] = output_df['breath_id'].map(u_in_half_mean_dict)\n",
    "#     u_in_half_std_dict = train.groupby('breath_id')['u_in_half'].std().to_dict()\n",
    "#     train['u_in_half_std'] = train['breath_id'].map(u_in_half_std_dict)\n",
    "\n",
    "    del output_df['u_in_half'], output_df['tmp']\n",
    "    return output_df.iloc[:, c_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(series, b, a):\n",
    "    return signal.filtfilt(b, a, series)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    fp = 5 # 通過域端周波数[Hz]\n",
    "    fs = 10 # 阻止域端周波数[Hz]\n",
    "    gpass = 3 # 通過域端最大損失[dB]\n",
    "    gstop = 40 # 阻止域端最小損失[dB]\n",
    "    samplerate = 100\n",
    "\n",
    "    fn = samplerate / 2   #ナイキスト周波数\n",
    "    wp = fp / fn  #ナイキスト周波数で通過域端周波数を正規化\n",
    "    ws = fs / fn  #ナイキスト周波数で阻止域端周波数を正規化\n",
    "    N, Wn = signal.buttord(wp, ws, gpass, gstop)  #オーダーとバターワースの正規化周波数を計算\n",
    "    b, a = signal.butter(N, Wn, \"low\")            #フィルタ伝達関数の分子と分母を計算\n",
    "    \n",
    "    def get_agg_window(start_time=0, end_time=3.0, add_suffix = False):\n",
    "        \n",
    "        df_tgt = output_df[(output_df['time_step'] >= start_time) & (output_df['time_step'] <= end_time)]\n",
    "        df_feature = df_tgt.groupby(['breath_id'])['u_in'].apply(lowpass_filter, b=b, a=a)\n",
    "        df_feature.name = 'u_in_filter'\n",
    "                    \n",
    "        return df_feature\n",
    "    \n",
    "    df_agg_feature = get_agg_window().reset_index()\n",
    "    df_agg_feature = df_agg_feature.explode(\"u_in_filter\").reset_index(drop=True)\n",
    "    df_agg_feature['u_in_filter'] = df_agg_feature['u_in_filter'].astype(float)\n",
    "        \n",
    "    df_agg_feature['u_in_filter_cumsum'] = df_agg_feature.groupby('breath_id')['u_in_filter'].cumsum()\n",
    "\n",
    "    return df_agg_feature.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vib_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    output_df['u_out_diff'] = output_df['u_out'].diff()\n",
    "    output_df['u_out_diff'].fillna(0, inplace=True)\n",
    "    output_df['u_out_diff'].replace(-1, 0, inplace=True)\n",
    "    uout1_df = output_df[output_df['u_out_diff']==1]\n",
    "    \n",
    "    first_df = output_df.loc[0::80,:]\n",
    "    first_0_dict = dict(zip(first_df['id'], [0]*len(uout1_df)))\n",
    "\n",
    "    output_df['u_in_diff'] = output_df['u_in'].diff()\n",
    "    output_df['diff_sign'] = np.sign(output_df['u_in_diff'])\n",
    "    output_df['sign_diff'] = output_df['diff_sign'].diff()\n",
    "    output_df['tmp'] = output_df['id'].map(first_0_dict) # put 0, the 80row cycle\n",
    "    output_df.iloc[0::80, output_df.columns.get_loc('sign_diff')] = output_df.iloc[0::80, output_df.columns.get_loc('tmp')]\n",
    "\n",
    "    # Count the number of inversions, so take the absolute value and sum\n",
    "    output_df['sign_diff'] = abs(output_df['sign_diff']) \n",
    "    sign_diff_dict = output_df.groupby('breath_id')['sign_diff'].sum().to_dict()\n",
    "    output_df['diff_vib'] = output_df['breath_id'].map(sign_diff_dict)\n",
    "    \n",
    "    return output_df['sign_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(out_df, input_df, dataType = 'train'):\n",
    "\n",
    "    USE_LAG = [-2, -1, 1, 2, 3, 4]\n",
    "    lag_map = {-2: 1, -1: 2, 1: 3, 2: 4, 3: 5, 4: 6}\n",
    "\n",
    "    out_df['breath_id'] = input_df['breath_id']\n",
    "    \n",
    "    for lag in USE_LAG:\n",
    "        out_df[f'breath_id_lag{lag_map[lag]}']=out_df['breath_id'].shift(lag).fillna(0)\n",
    "        out_df[f'breath_id_lag{lag_map[lag]}same']=np.select([out_df[f'breath_id_lag{lag_map[lag]}']==out_df['breath_id']], [1], 0)\n",
    "\n",
    "        # u_in_filter\n",
    "        out_df[f'u_in_filter_lag_{lag_map[lag]}'] = out_df['u_in_filter'].shift(lag).fillna(0) * out_df[f'breath_id_lag{lag_map[lag]}same']\n",
    "        out_df[f'u_in_filter_diff_{lag_map[lag]}'] = out_df['u_in_filter'] - out_df[f'u_in_filter_lag_{lag_map[lag]}']\n",
    "        # u_in_sqrt\n",
    "        out_df[f'u_in_sqrt_lag_{lag_map[lag]}'] = out_df['u_in_sqrt'].shift(lag).fillna(0) * out_df[f'breath_id_lag{lag_map[lag]}same']\n",
    "        out_df[f'u_in_sqrt_diff_{lag_map[lag]}'] = out_df['u_in_sqrt'] - out_df[f'u_in_sqrt_lag_{lag_map[lag]}']\n",
    "\n",
    "        # u_in \n",
    "        out_df[f'u_in_lag_{lag_map[lag]}'] = out_df['u_in'].shift(lag).fillna(0) * out_df[f'breath_id_lag{lag_map[lag]}same']\n",
    "        out_df[f'u_in_diff_{lag_map[lag]}'] = out_df['u_in'] - out_df[f'u_in_lag_{lag_map[lag]}']\n",
    "        # u_out\n",
    "        out_df[f'u_out_lag_{lag_map[lag]}'] = out_df['u_out'].shift(lag).fillna(0) * out_df[f'breath_id_lag{lag_map[lag]}same']\n",
    "\n",
    "        # breath_time\n",
    "    out_df[f'time_step_lag_{1}'] = out_df['time_step'].shift(1).fillna(0) * out_df[f'breath_id_lag{1}same']\n",
    "    out_df[f'time_step_diff_{1}'] = out_df['time_step'] - out_df[f'time_step_lag_{1}']\n",
    "        \n",
    "    drop_columns = ['breath_id', 'time_step_lag_1']\n",
    "    drop_columns += [f'breath_id_lag{lag_map[i]}' for i in USE_LAG]\n",
    "    drop_columns += [f'breath_id_lag{lag_map[i]}same' for i in USE_LAG]\n",
    "    out_df = out_df.drop(drop_columns, axis=1)\n",
    "    out_df = out_df.fillna(0)\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    output_df = copy.deepcopy(input_df)\n",
    "    c_num = input_df.shape[1]\n",
    "    \n",
    "    for i in range(len(pred_cols)):\n",
    "        output_df[f\"pred_{i}\"] = 0.\n",
    "        output_df.loc[oof[\"u_out\"] == 0, f\"pred_{i}\"] = _oof[f\"oof{i}\"].values\n",
    "    \n",
    "\n",
    "    \n",
    "    return output_df['sign_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature(input_df, dataType = 'train'):\n",
    "    \"\"\"input_df を特徴量行列に変換した新しいデータフレームを返す.\n",
    "    \"\"\"\n",
    "\n",
    "    processors = [\n",
    "        get_raw_features,\n",
    "#         get_simple_calc_features,\n",
    "#         get_agg_features,\n",
    "#         get_vib_features,\n",
    "#         get_half_features,\n",
    "        get_category_features,\n",
    "#         get_filter_features,\n",
    "    ]\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    for func in tqdm(processors, total=len(processors)):\n",
    "        with Timer(prefix='' + func.__name__ + ' '):\n",
    "            _df = func(input_df, dataType)\n",
    "\n",
    "        # 長さが等しいことをチェック (ずれている場合, func の実装がおかしい)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "#     out_df = utils.reduce_mem_usage(out_df)\n",
    "#     out_df = add_time_features(out_df, input_df)\n",
    "    out_df_cols = sorted(list(out_df))\n",
    "    out_df = out_df[out_df_cols]\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_raw_features  0.027[s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_category_features  2.336[s]\n",
      "get_raw_features  0.020[s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_category_features  1.537[s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = to_feature(train, dataType = 'train')\n",
    "test_df = to_feature(test, dataType = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [f\"pred_{i}\" for i in range(len(oofs.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_oof_features(input_df, dataType = 'train'):\n",
    "    \n",
    "    if dataType == 'train':\n",
    "        for i in range(len(pred_cols)):\n",
    "            input_df[f\"pred_{i}\"] = 0.\n",
    "            input_df.loc[train[\"u_out\"] == 0, f\"pred_{i}\"] = oofs[f\"pred_{i}\"].values\n",
    "        input_df['breath_id'] = train['breath_id']\n",
    "        input_df = train_df.loc[train[\"u_out\"] == 0].reset_index(drop=True)\n",
    "    else:\n",
    "        for i in range(len(pred_cols)):\n",
    "            input_df[f\"pred_{i}\"] = 0.\n",
    "            input_df.loc[:, f\"pred_{i}\"] = subs[f\"pred_{i}\"].values\n",
    "        input_df['breath_id'] = test['breath_id']\n",
    "        input_df = input_df.loc[test[\"u_out\"] == 0].reset_index(drop=True)\n",
    "      \n",
    "    # v2\n",
    "    input_df[\"pred_mean\"] = np.mean(input_df[pred_cols].values, axis=1)\n",
    "    input_df[\"pred_median\"] = np.median(input_df[pred_cols].values, axis=1)\n",
    "\n",
    "    input_df[\"pred_std\"] = input_df[pred_cols].std(axis=1)\n",
    "    input_df[\"pred_max\"] = input_df[pred_cols].values.max(axis=1)\n",
    "    input_df[\"pred_min\"] = input_df[pred_cols].values.min(axis=1)\n",
    "    input_df[\"pred_max-min\"] = input_df[\"pred_max\"] - input_df[\"pred_min\"]\n",
    "    input_df[\"pred_max-median\"] = input_df[\"pred_max\"] - input_df[\"pred_median\"]\n",
    "    input_df[\"pred_max-mean\"] = input_df[\"pred_max\"] - input_df[\"pred_mean\"]\n",
    "    input_df[\"pred_median-min\"] = input_df[\"pred_median\"] - input_df[\"pred_min\"]\n",
    "    input_df[\"pred_mean-min\"] = input_df[\"pred_mean\"] - input_df[\"pred_min\"]\n",
    "    input_df[\"pred_mean-median\"] = input_df[\"pred_mean\"] - input_df[\"pred_median\"]\n",
    "    input_df[\"pred_kurt\"] = input_df[pred_cols].kurt(axis=1)\n",
    "    for col_ in pred_cols:\n",
    "        input_df[f\"{col_}_past_1\"] = input_df.groupby(\"breath_id\")[f\"{col_}\"].shift(1)\n",
    "        input_df[f\"{col_}_past_2\"] = input_df.groupby(\"breath_id\")[f\"{col_}\"].shift(2)\n",
    "        input_df[f\"{col_}_past_3\"] = input_df.groupby(\"breath_id\")[f\"{col_}\"].shift(3)\n",
    "        input_df[f\"{col_}_past_4\"] = input_df.groupby(\"breath_id\")[f\"{col_}\"].shift(4)\n",
    "\n",
    "        input_df[f\"{col_}_diff_1\"] = input_df[f\"{col_}\"] - input_df[f\"{col_}_past_1\"]\n",
    "        input_df[f\"{col_}_diff_2\"] = input_df[f\"{col_}\"] - input_df[f\"{col_}_past_2\"]\n",
    "        input_df[f\"{col_}_diff_3\"] = input_df[f\"{col_}\"] - input_df[f\"{col_}_past_3\"]\n",
    "        input_df[f\"{col_}_diff_4\"] = input_df[f\"{col_}\"] - input_df[f\"{col_}_past_4\"]\n",
    "\n",
    "    input_df[\"u_in_past_1\"] = input_df.groupby(\"breath_id\")[\"u_in\"].shift(1)\n",
    "    input_df[\"u_in_past_2\"] = input_df.groupby(\"breath_id\")[\"u_in\"].shift(2)\n",
    "    input_df[\"u_in_past_3\"] = input_df.groupby(\"breath_id\")[\"u_in\"].shift(3)\n",
    "    input_df[\"u_in_past_4\"] = input_df.groupby(\"breath_id\")[\"u_in\"].shift(4)\n",
    "\n",
    "    input_df[\"u_in_diff_1\"] = input_df[\"u_in\"] - input_df[\"u_in_past_1\"]\n",
    "    input_df[\"u_in_diff_2\"] = input_df[\"u_in\"] - input_df[\"u_in_past_2\"]\n",
    "    input_df[\"u_in_diff_3\"] = input_df[\"u_in\"] - input_df[\"u_in_past_3\"]\n",
    "    input_df[\"u_in_diff_4\"] = input_df[\"u_in\"] - input_df[\"u_in_past_4\"]\n",
    "\n",
    "    input_df[\"u_in_cumsum\"] = input_df.groupby(\"breath_id\")[\"u_in\"].cumsum()\n",
    "\n",
    "    del input_df['breath_id']\n",
    "    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_oof_features(train_df, dataType = 'train')\n",
    "test_df = add_oof_features(test_df, dataType = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>R</th>\n",
       "      <th>R_C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_19_diff_4</th>\n",
       "      <th>u_in_past_1</th>\n",
       "      <th>u_in_past_2</th>\n",
       "      <th>u_in_past_3</th>\n",
       "      <th>u_in_past_4</th>\n",
       "      <th>u_in_diff_1</th>\n",
       "      <th>u_in_diff_2</th>\n",
       "      <th>u_in_diff_3</th>\n",
       "      <th>u_in_diff_4</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>5.843223</td>\n",
       "      <td>6.287325</td>\n",
       "      <td>5.828472</td>\n",
       "      <td>5.783465</td>\n",
       "      <td>6.013763</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>5.873457</td>\n",
       "      <td>5.989612</td>\n",
       "      <td>5.880560</td>\n",
       "      <td>5.869486</td>\n",
       "      <td>6.103593</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.299707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.466375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>7.892688</td>\n",
       "      <td>8.725241</td>\n",
       "      <td>8.157262</td>\n",
       "      <td>7.865620</td>\n",
       "      <td>8.852818</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.126236</td>\n",
       "      <td>22.425944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.975653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>11.861191</td>\n",
       "      <td>12.752718</td>\n",
       "      <td>12.129442</td>\n",
       "      <td>11.779996</td>\n",
       "      <td>12.312957</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>4.425781</td>\n",
       "      <td>22.725488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.784476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>12.331974</td>\n",
       "      <td>12.974612</td>\n",
       "      <td>12.576923</td>\n",
       "      <td>12.318006</td>\n",
       "      <td>12.656638</td>\n",
       "      <td>...</td>\n",
       "      <td>6.173177</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>2.547028</td>\n",
       "      <td>2.846573</td>\n",
       "      <td>6.972809</td>\n",
       "      <td>25.272516</td>\n",
       "      <td>89.140326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290963</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>0.834147</td>\n",
       "      <td>1.869367</td>\n",
       "      <td>29.492047</td>\n",
       "      <td>29.437429</td>\n",
       "      <td>29.562796</td>\n",
       "      <td>29.377239</td>\n",
       "      <td>29.468651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973099</td>\n",
       "      <td>2.650333</td>\n",
       "      <td>2.438287</td>\n",
       "      <td>3.783043</td>\n",
       "      <td>3.209590</td>\n",
       "      <td>-0.780965</td>\n",
       "      <td>-0.568920</td>\n",
       "      <td>-1.913676</td>\n",
       "      <td>-1.340223</td>\n",
       "      <td>238.890288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290964</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>0.867574</td>\n",
       "      <td>2.154414</td>\n",
       "      <td>29.098768</td>\n",
       "      <td>29.094450</td>\n",
       "      <td>29.233667</td>\n",
       "      <td>29.051178</td>\n",
       "      <td>29.123398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264233</td>\n",
       "      <td>1.869367</td>\n",
       "      <td>2.650333</td>\n",
       "      <td>2.438287</td>\n",
       "      <td>3.783043</td>\n",
       "      <td>0.285047</td>\n",
       "      <td>-0.495918</td>\n",
       "      <td>-0.283873</td>\n",
       "      <td>-1.628629</td>\n",
       "      <td>241.044703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290965</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>0.900917</td>\n",
       "      <td>1.304434</td>\n",
       "      <td>29.777598</td>\n",
       "      <td>29.816580</td>\n",
       "      <td>29.977200</td>\n",
       "      <td>29.847507</td>\n",
       "      <td>29.894068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788420</td>\n",
       "      <td>2.154414</td>\n",
       "      <td>1.869367</td>\n",
       "      <td>2.650333</td>\n",
       "      <td>2.438287</td>\n",
       "      <td>-0.849980</td>\n",
       "      <td>-0.564933</td>\n",
       "      <td>-1.345899</td>\n",
       "      <td>-1.133853</td>\n",
       "      <td>242.349137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290966</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>0.934309</td>\n",
       "      <td>1.733830</td>\n",
       "      <td>29.344897</td>\n",
       "      <td>29.375248</td>\n",
       "      <td>29.443771</td>\n",
       "      <td>29.373464</td>\n",
       "      <td>29.389355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580425</td>\n",
       "      <td>1.304434</td>\n",
       "      <td>2.154414</td>\n",
       "      <td>1.869367</td>\n",
       "      <td>2.650333</td>\n",
       "      <td>0.429396</td>\n",
       "      <td>-0.420585</td>\n",
       "      <td>-0.135538</td>\n",
       "      <td>-0.916503</td>\n",
       "      <td>244.082966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290967</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>0.967743</td>\n",
       "      <td>0.958726</td>\n",
       "      <td>29.955917</td>\n",
       "      <td>30.020460</td>\n",
       "      <td>30.144281</td>\n",
       "      <td>30.003941</td>\n",
       "      <td>30.114656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580046</td>\n",
       "      <td>1.733830</td>\n",
       "      <td>1.304434</td>\n",
       "      <td>2.154414</td>\n",
       "      <td>1.869367</td>\n",
       "      <td>-0.775103</td>\n",
       "      <td>-0.345708</td>\n",
       "      <td>-1.195688</td>\n",
       "      <td>-0.910641</td>\n",
       "      <td>245.041693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2290968 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          C   R  R_C  time_step       u_in     pred_0     pred_1     pred_2  \\\n",
       "0        50  20    5   0.000000   0.083334   5.843223   6.287325   5.828472   \n",
       "1        50  20    5   0.033652  18.383041   5.873457   5.989612   5.880560   \n",
       "2        50  20    5   0.067514  22.509278   7.892688   8.725241   8.157262   \n",
       "3        50  20    5   0.101542  22.808822  11.861191  12.752718  12.129442   \n",
       "4        50  20    5   0.135756  25.355850  12.331974  12.974612  12.576923   \n",
       "...      ..  ..  ...        ...        ...        ...        ...        ...   \n",
       "2290963  10  50    6   0.834147   1.869367  29.492047  29.437429  29.562796   \n",
       "2290964  10  50    6   0.867574   2.154414  29.098768  29.094450  29.233667   \n",
       "2290965  10  50    6   0.900917   1.304434  29.777598  29.816580  29.977200   \n",
       "2290966  10  50    6   0.934309   1.733830  29.344897  29.375248  29.443771   \n",
       "2290967  10  50    6   0.967743   0.958726  29.955917  30.020460  30.144281   \n",
       "\n",
       "            pred_3     pred_4  ...  pred_19_diff_4  u_in_past_1  u_in_past_2  \\\n",
       "0         5.783465   6.013763  ...             NaN          NaN          NaN   \n",
       "1         5.869486   6.103593  ...             NaN     0.083334          NaN   \n",
       "2         7.865620   8.852818  ...             NaN    18.383041     0.083334   \n",
       "3        11.779996  12.312957  ...             NaN    22.509278    18.383041   \n",
       "4        12.318006  12.656638  ...        6.173177    22.808822    22.509278   \n",
       "...            ...        ...  ...             ...          ...          ...   \n",
       "2290963  29.377239  29.468651  ...        0.973099     2.650333     2.438287   \n",
       "2290964  29.051178  29.123398  ...        1.264233     1.869367     2.650333   \n",
       "2290965  29.847507  29.894068  ...        0.788420     2.154414     1.869367   \n",
       "2290966  29.373464  29.389355  ...        0.580425     1.304434     2.154414   \n",
       "2290967  30.003941  30.114656  ...        0.580046     1.733830     1.304434   \n",
       "\n",
       "         u_in_past_3  u_in_past_4  u_in_diff_1  u_in_diff_2  u_in_diff_3  \\\n",
       "0                NaN          NaN          NaN          NaN          NaN   \n",
       "1                NaN          NaN    18.299707          NaN          NaN   \n",
       "2                NaN          NaN     4.126236    22.425944          NaN   \n",
       "3           0.083334          NaN     0.299544     4.425781    22.725488   \n",
       "4          18.383041     0.083334     2.547028     2.846573     6.972809   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "2290963     3.783043     3.209590    -0.780965    -0.568920    -1.913676   \n",
       "2290964     2.438287     3.783043     0.285047    -0.495918    -0.283873   \n",
       "2290965     2.650333     2.438287    -0.849980    -0.564933    -1.345899   \n",
       "2290966     1.869367     2.650333     0.429396    -0.420585    -0.135538   \n",
       "2290967     2.154414     1.869367    -0.775103    -0.345708    -1.195688   \n",
       "\n",
       "         u_in_diff_4  u_in_cumsum  \n",
       "0                NaN     0.083334  \n",
       "1                NaN    18.466375  \n",
       "2                NaN    40.975653  \n",
       "3                NaN    63.784476  \n",
       "4          25.272516    89.140326  \n",
       "...              ...          ...  \n",
       "2290963    -1.340223   238.890288  \n",
       "2290964    -1.628629   241.044703  \n",
       "2290965    -1.133853   242.349137  \n",
       "2290966    -0.916503   244.082966  \n",
       "2290967    -0.910641   245.041693  \n",
       "\n",
       "[2290968 rows x 206 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>R</th>\n",
       "      <th>R_C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_19_diff_4</th>\n",
       "      <th>u_in_past_1</th>\n",
       "      <th>u_in_past_2</th>\n",
       "      <th>u_in_past_3</th>\n",
       "      <th>u_in_past_4</th>\n",
       "      <th>u_in_diff_1</th>\n",
       "      <th>u_in_diff_2</th>\n",
       "      <th>u_in_diff_3</th>\n",
       "      <th>u_in_diff_4</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.237001</td>\n",
       "      <td>6.190098</td>\n",
       "      <td>6.258121</td>\n",
       "      <td>6.315998</td>\n",
       "      <td>6.157633</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031904</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>5.938651</td>\n",
       "      <td>6.075787</td>\n",
       "      <td>5.953184</td>\n",
       "      <td>5.938564</td>\n",
       "      <td>6.029568</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.515046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>14.651675</td>\n",
       "      <td>7.218726</td>\n",
       "      <td>7.236039</td>\n",
       "      <td>7.180656</td>\n",
       "      <td>7.115508</td>\n",
       "      <td>7.136393</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.136630</td>\n",
       "      <td>14.651675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.166721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>21.230610</td>\n",
       "      <td>7.752637</td>\n",
       "      <td>7.880373</td>\n",
       "      <td>7.789594</td>\n",
       "      <td>7.607679</td>\n",
       "      <td>7.800501</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.651675</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.578935</td>\n",
       "      <td>13.715564</td>\n",
       "      <td>21.230610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.397331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127644</td>\n",
       "      <td>26.320956</td>\n",
       "      <td>9.262370</td>\n",
       "      <td>9.373093</td>\n",
       "      <td>9.275614</td>\n",
       "      <td>9.145709</td>\n",
       "      <td>9.164462</td>\n",
       "      <td>...</td>\n",
       "      <td>3.164127</td>\n",
       "      <td>21.230610</td>\n",
       "      <td>14.651675</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.090346</td>\n",
       "      <td>11.669281</td>\n",
       "      <td>18.805911</td>\n",
       "      <td>26.320956</td>\n",
       "      <td>69.718287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527560</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.842145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.120168</td>\n",
       "      <td>10.109750</td>\n",
       "      <td>10.103710</td>\n",
       "      <td>10.101749</td>\n",
       "      <td>10.101664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.839645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527561</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.102587</td>\n",
       "      <td>10.094096</td>\n",
       "      <td>10.086064</td>\n",
       "      <td>10.093045</td>\n",
       "      <td>10.088296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.839645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527562</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909185</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>9.986692</td>\n",
       "      <td>9.984508</td>\n",
       "      <td>9.993743</td>\n",
       "      <td>9.985178</td>\n",
       "      <td>9.984118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>68.961019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527563</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.943148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.080103</td>\n",
       "      <td>10.081043</td>\n",
       "      <td>10.095322</td>\n",
       "      <td>10.078216</td>\n",
       "      <td>10.082279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028583</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.961019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527564</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.073535</td>\n",
       "      <td>10.070534</td>\n",
       "      <td>10.085063</td>\n",
       "      <td>10.078068</td>\n",
       "      <td>10.076832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.121375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.961019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1527565 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          C   R  R_C  time_step       u_in     pred_0     pred_1     pred_2  \\\n",
       "0        20   5    1   0.000000   0.000000   6.237001   6.190098   6.258121   \n",
       "1        20   5    1   0.031904   7.515046   5.938651   6.075787   5.953184   \n",
       "2        20   5    1   0.063827  14.651675   7.218726   7.236039   7.180656   \n",
       "3        20   5    1   0.095751  21.230610   7.752637   7.880373   7.789594   \n",
       "4        20   5    1   0.127644  26.320956   9.262370   9.373093   9.275614   \n",
       "...      ..  ..  ...        ...        ...        ...        ...        ...   \n",
       "1527560  10  20    3   0.842145   0.000000  10.120168  10.109750  10.103710   \n",
       "1527561  10  20    3   0.875648   0.000000  10.102587  10.094096  10.086064   \n",
       "1527562  10  20    3   0.909185   0.121375   9.986692   9.984508   9.993743   \n",
       "1527563  10  20    3   0.943148   0.000000  10.080103  10.081043  10.095322   \n",
       "1527564  10  20    3   0.976815   0.000000  10.073535  10.070534  10.085063   \n",
       "\n",
       "            pred_3     pred_4  ...  pred_19_diff_4  u_in_past_1  u_in_past_2  \\\n",
       "0         6.315998   6.157633  ...             NaN          NaN          NaN   \n",
       "1         5.938564   6.029568  ...             NaN     0.000000          NaN   \n",
       "2         7.115508   7.136393  ...             NaN     7.515046     0.000000   \n",
       "3         7.607679   7.800501  ...             NaN    14.651675     7.515046   \n",
       "4         9.145709   9.164462  ...        3.164127    21.230610    14.651675   \n",
       "...            ...        ...  ...             ...          ...          ...   \n",
       "1527560  10.101749  10.101664  ...        0.014219     0.000000     0.000000   \n",
       "1527561  10.093045  10.088296  ...       -0.042123     0.000000     0.000000   \n",
       "1527562   9.985178   9.984118  ...       -0.168992     0.000000     0.000000   \n",
       "1527563  10.078216  10.082279  ...       -0.028583     0.121375     0.000000   \n",
       "1527564  10.078068  10.076832  ...       -0.037798     0.000000     0.121375   \n",
       "\n",
       "         u_in_past_3  u_in_past_4  u_in_diff_1  u_in_diff_2  u_in_diff_3  \\\n",
       "0                NaN          NaN          NaN          NaN          NaN   \n",
       "1                NaN          NaN     7.515046          NaN          NaN   \n",
       "2                NaN          NaN     7.136630    14.651675          NaN   \n",
       "3           0.000000          NaN     6.578935    13.715564    21.230610   \n",
       "4           7.515046          0.0     5.090346    11.669281    18.805911   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "1527560     0.000000          0.0     0.000000     0.000000     0.000000   \n",
       "1527561     0.000000          0.0     0.000000     0.000000     0.000000   \n",
       "1527562     0.000000          0.0     0.121375     0.121375     0.121375   \n",
       "1527563     0.000000          0.0    -0.121375     0.000000     0.000000   \n",
       "1527564     0.000000          0.0     0.000000    -0.121375     0.000000   \n",
       "\n",
       "         u_in_diff_4  u_in_cumsum  \n",
       "0                NaN     0.000000  \n",
       "1                NaN     7.515046  \n",
       "2                NaN    22.166721  \n",
       "3                NaN    43.397331  \n",
       "4          26.320956    69.718287  \n",
       "...              ...          ...  \n",
       "1527560     0.000000    68.839645  \n",
       "1527561     0.000000    68.839645  \n",
       "1527562     0.121375    68.961019  \n",
       "1527563     0.000000    68.961019  \n",
       "1527564     0.000000    68.961019  \n",
       "\n",
       "[1527565 rows x 206 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(train_df), display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_col = ['R_C']\n",
    "train_value_col = [i for i in train_df.columns.to_list() if i not in train_category_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_features = train_value_col\n",
    "# train_df = train_df.fillna(0)\n",
    "# test_df = test_df.fillna(0)\n",
    "# scaler = RobustScaler()\n",
    "# scaler.fit(train_df[norm_features])\n",
    "# train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n",
    "# test_df[norm_features] = scaler.transform(test_df[norm_features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.loc[train[\"u_out\"] == 0]['pressure'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train.loc[train[\"u_out\"] == 0, ['id', 'breath_id', 'pressure']].reset_index(drop=True)], axis=1)\n",
    "test_df = pd.concat([test_df, test.loc[test[\"u_out\"] == 0,['id', 'breath_id']].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-0\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.440547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061893, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.134218\tvalid_1's l1: 0.133831\n",
      "[200]\ttraining's l1: 0.125723\tvalid_1's l1: 0.12516\n",
      "[300]\ttraining's l1: 0.123129\tvalid_1's l1: 0.12257\n",
      "[400]\ttraining's l1: 0.12167\tvalid_1's l1: 0.121344\n",
      "[500]\ttraining's l1: 0.120604\tvalid_1's l1: 0.120575\n",
      "[600]\ttraining's l1: 0.119891\tvalid_1's l1: 0.120091\n",
      "[700]\ttraining's l1: 0.119246\tvalid_1's l1: 0.119624\n",
      "[800]\ttraining's l1: 0.118755\tvalid_1's l1: 0.119335\n",
      "[900]\ttraining's l1: 0.118344\tvalid_1's l1: 0.1191\n",
      "[1000]\ttraining's l1: 0.117959\tvalid_1's l1: 0.118916\n",
      "[1100]\ttraining's l1: 0.117646\tvalid_1's l1: 0.118784\n",
      "[1200]\ttraining's l1: 0.117308\tvalid_1's l1: 0.118667\n",
      "[1300]\ttraining's l1: 0.116988\tvalid_1's l1: 0.118573\n",
      "[1400]\ttraining's l1: 0.11675\tvalid_1's l1: 0.118455\n",
      "[1500]\ttraining's l1: 0.116574\tvalid_1's l1: 0.118405\n",
      "[1600]\ttraining's l1: 0.116348\tvalid_1's l1: 0.118329\n",
      "[1700]\ttraining's l1: 0.116149\tvalid_1's l1: 0.118267\n",
      "[1800]\ttraining's l1: 0.115997\tvalid_1's l1: 0.118215\n",
      "[1900]\ttraining's l1: 0.115784\tvalid_1's l1: 0.118134\n",
      "[2000]\ttraining's l1: 0.115583\tvalid_1's l1: 0.118079\n",
      "[2100]\ttraining's l1: 0.115414\tvalid_1's l1: 0.118037\n",
      "[2200]\ttraining's l1: 0.115276\tvalid_1's l1: 0.118002\n",
      "[2300]\ttraining's l1: 0.115152\tvalid_1's l1: 0.117961\n",
      "[2400]\ttraining's l1: 0.115009\tvalid_1's l1: 0.117914\n",
      "[2500]\ttraining's l1: 0.114862\tvalid_1's l1: 0.11788\n",
      "[2600]\ttraining's l1: 0.114725\tvalid_1's l1: 0.117854\n",
      "[2700]\ttraining's l1: 0.114519\tvalid_1's l1: 0.117826\n",
      "[2800]\ttraining's l1: 0.114363\tvalid_1's l1: 0.117793\n",
      "[2900]\ttraining's l1: 0.114227\tvalid_1's l1: 0.117763\n",
      "[3000]\ttraining's l1: 0.114088\tvalid_1's l1: 0.117743\n",
      "[3100]\ttraining's l1: 0.113979\tvalid_1's l1: 0.117717\n",
      "[3200]\ttraining's l1: 0.113846\tvalid_1's l1: 0.117704\n",
      "[3300]\ttraining's l1: 0.113757\tvalid_1's l1: 0.11769\n",
      "[3400]\ttraining's l1: 0.113656\tvalid_1's l1: 0.117672\n",
      "[3500]\ttraining's l1: 0.113547\tvalid_1's l1: 0.117648\n",
      "[3600]\ttraining's l1: 0.113455\tvalid_1's l1: 0.11762\n",
      "[3700]\ttraining's l1: 0.113337\tvalid_1's l1: 0.117614\n",
      "[3800]\ttraining's l1: 0.113236\tvalid_1's l1: 0.117602\n",
      "[3900]\ttraining's l1: 0.113126\tvalid_1's l1: 0.117587\n",
      "[4000]\ttraining's l1: 0.113008\tvalid_1's l1: 0.117596\n",
      "Early stopping, best iteration is:\n",
      "[3858]\ttraining's l1: 0.113183\tvalid_1's l1: 0.117584\n",
      "fold = 0, score = 0.11758392880907353\n",
      "Fold-1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.455440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061993, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.132209\tvalid_1's l1: 0.134607\n",
      "[200]\ttraining's l1: 0.125798\tvalid_1's l1: 0.127889\n",
      "[300]\ttraining's l1: 0.123362\tvalid_1's l1: 0.125493\n",
      "[400]\ttraining's l1: 0.121198\tvalid_1's l1: 0.123487\n",
      "[500]\ttraining's l1: 0.120139\tvalid_1's l1: 0.122778\n",
      "[600]\ttraining's l1: 0.119302\tvalid_1's l1: 0.122161\n",
      "[700]\ttraining's l1: 0.11868\tvalid_1's l1: 0.121825\n",
      "[800]\ttraining's l1: 0.118149\tvalid_1's l1: 0.121568\n",
      "[900]\ttraining's l1: 0.117769\tvalid_1's l1: 0.121284\n",
      "[1000]\ttraining's l1: 0.117425\tvalid_1's l1: 0.121109\n",
      "[1100]\ttraining's l1: 0.117025\tvalid_1's l1: 0.120873\n",
      "[1200]\ttraining's l1: 0.116725\tvalid_1's l1: 0.120725\n",
      "[1300]\ttraining's l1: 0.116466\tvalid_1's l1: 0.120617\n",
      "[1400]\ttraining's l1: 0.116233\tvalid_1's l1: 0.120532\n",
      "[1500]\ttraining's l1: 0.116012\tvalid_1's l1: 0.120433\n",
      "[1600]\ttraining's l1: 0.115793\tvalid_1's l1: 0.120347\n",
      "[1700]\ttraining's l1: 0.115585\tvalid_1's l1: 0.120265\n",
      "[1800]\ttraining's l1: 0.115384\tvalid_1's l1: 0.1202\n",
      "[1900]\ttraining's l1: 0.115213\tvalid_1's l1: 0.120124\n",
      "[2000]\ttraining's l1: 0.115058\tvalid_1's l1: 0.120084\n",
      "[2100]\ttraining's l1: 0.114908\tvalid_1's l1: 0.120029\n",
      "[2200]\ttraining's l1: 0.114772\tvalid_1's l1: 0.119987\n",
      "[2300]\ttraining's l1: 0.114622\tvalid_1's l1: 0.119944\n",
      "[2400]\ttraining's l1: 0.114464\tvalid_1's l1: 0.119912\n",
      "[2500]\ttraining's l1: 0.114314\tvalid_1's l1: 0.119858\n",
      "[2600]\ttraining's l1: 0.114161\tvalid_1's l1: 0.119821\n",
      "[2700]\ttraining's l1: 0.114034\tvalid_1's l1: 0.119795\n",
      "[2800]\ttraining's l1: 0.113922\tvalid_1's l1: 0.119768\n",
      "[2900]\ttraining's l1: 0.113797\tvalid_1's l1: 0.119746\n",
      "[3000]\ttraining's l1: 0.113668\tvalid_1's l1: 0.119701\n",
      "[3100]\ttraining's l1: 0.113546\tvalid_1's l1: 0.11967\n",
      "[3200]\ttraining's l1: 0.113406\tvalid_1's l1: 0.119637\n",
      "[3300]\ttraining's l1: 0.113298\tvalid_1's l1: 0.119611\n",
      "[3400]\ttraining's l1: 0.11317\tvalid_1's l1: 0.119583\n",
      "[3500]\ttraining's l1: 0.113083\tvalid_1's l1: 0.119578\n",
      "[3600]\ttraining's l1: 0.112971\tvalid_1's l1: 0.119542\n",
      "[3700]\ttraining's l1: 0.112887\tvalid_1's l1: 0.119524\n",
      "[3800]\ttraining's l1: 0.112799\tvalid_1's l1: 0.119514\n",
      "[3900]\ttraining's l1: 0.112723\tvalid_1's l1: 0.119493\n",
      "[4000]\ttraining's l1: 0.112629\tvalid_1's l1: 0.119486\n",
      "[4100]\ttraining's l1: 0.112549\tvalid_1's l1: 0.11948\n",
      "[4200]\ttraining's l1: 0.112479\tvalid_1's l1: 0.119481\n",
      "[4300]\ttraining's l1: 0.112391\tvalid_1's l1: 0.11946\n",
      "[4400]\ttraining's l1: 0.112317\tvalid_1's l1: 0.119465\n",
      "[4500]\ttraining's l1: 0.112228\tvalid_1's l1: 0.119445\n",
      "[4600]\ttraining's l1: 0.112139\tvalid_1's l1: 0.119436\n",
      "[4700]\ttraining's l1: 0.112035\tvalid_1's l1: 0.119426\n",
      "[4800]\ttraining's l1: 0.111959\tvalid_1's l1: 0.119421\n",
      "[4900]\ttraining's l1: 0.111885\tvalid_1's l1: 0.119408\n",
      "[5000]\ttraining's l1: 0.111802\tvalid_1's l1: 0.119396\n",
      "[5100]\ttraining's l1: 0.111724\tvalid_1's l1: 0.119399\n",
      "Early stopping, best iteration is:\n",
      "[4978]\ttraining's l1: 0.111819\tvalid_1's l1: 0.119391\n",
      "fold = 1, score = 0.11939144670241485\n",
      "Fold-2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.440294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061937, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.133152\tvalid_1's l1: 0.130353\n",
      "[200]\ttraining's l1: 0.125569\tvalid_1's l1: 0.123398\n",
      "[300]\ttraining's l1: 0.123359\tvalid_1's l1: 0.121362\n",
      "[400]\ttraining's l1: 0.121709\tvalid_1's l1: 0.120078\n",
      "[500]\ttraining's l1: 0.120451\tvalid_1's l1: 0.119256\n",
      "[600]\ttraining's l1: 0.119675\tvalid_1's l1: 0.118814\n",
      "[700]\ttraining's l1: 0.119027\tvalid_1's l1: 0.118458\n",
      "[800]\ttraining's l1: 0.118547\tvalid_1's l1: 0.118238\n",
      "[900]\ttraining's l1: 0.118082\tvalid_1's l1: 0.118031\n",
      "[1000]\ttraining's l1: 0.117769\tvalid_1's l1: 0.117906\n",
      "[1100]\ttraining's l1: 0.117427\tvalid_1's l1: 0.117805\n",
      "[1200]\ttraining's l1: 0.117117\tvalid_1's l1: 0.117668\n",
      "[1300]\ttraining's l1: 0.116838\tvalid_1's l1: 0.117554\n",
      "[1400]\ttraining's l1: 0.116594\tvalid_1's l1: 0.11745\n",
      "[1500]\ttraining's l1: 0.116367\tvalid_1's l1: 0.11737\n",
      "[1600]\ttraining's l1: 0.11617\tvalid_1's l1: 0.117296\n",
      "[1700]\ttraining's l1: 0.115949\tvalid_1's l1: 0.11723\n",
      "[1800]\ttraining's l1: 0.11575\tvalid_1's l1: 0.117164\n",
      "[1900]\ttraining's l1: 0.115561\tvalid_1's l1: 0.117077\n",
      "[2000]\ttraining's l1: 0.115387\tvalid_1's l1: 0.117029\n",
      "[2100]\ttraining's l1: 0.115247\tvalid_1's l1: 0.117005\n",
      "[2200]\ttraining's l1: 0.115109\tvalid_1's l1: 0.116981\n",
      "[2300]\ttraining's l1: 0.114938\tvalid_1's l1: 0.116956\n",
      "[2400]\ttraining's l1: 0.114801\tvalid_1's l1: 0.116927\n",
      "[2500]\ttraining's l1: 0.11463\tvalid_1's l1: 0.116871\n",
      "[2600]\ttraining's l1: 0.114506\tvalid_1's l1: 0.116854\n",
      "[2700]\ttraining's l1: 0.114367\tvalid_1's l1: 0.116831\n",
      "[2800]\ttraining's l1: 0.114198\tvalid_1's l1: 0.116789\n",
      "[2900]\ttraining's l1: 0.114087\tvalid_1's l1: 0.116774\n",
      "[3000]\ttraining's l1: 0.113971\tvalid_1's l1: 0.116762\n",
      "[3100]\ttraining's l1: 0.113842\tvalid_1's l1: 0.116727\n",
      "[3200]\ttraining's l1: 0.113742\tvalid_1's l1: 0.116711\n",
      "[3300]\ttraining's l1: 0.113617\tvalid_1's l1: 0.116684\n",
      "[3400]\ttraining's l1: 0.113515\tvalid_1's l1: 0.116674\n",
      "[3500]\ttraining's l1: 0.113398\tvalid_1's l1: 0.116645\n",
      "[3600]\ttraining's l1: 0.113281\tvalid_1's l1: 0.116625\n",
      "[3700]\ttraining's l1: 0.113171\tvalid_1's l1: 0.116597\n",
      "[3800]\ttraining's l1: 0.113052\tvalid_1's l1: 0.116588\n",
      "[3900]\ttraining's l1: 0.112946\tvalid_1's l1: 0.116573\n",
      "[4000]\ttraining's l1: 0.112846\tvalid_1's l1: 0.116567\n",
      "[4100]\ttraining's l1: 0.112742\tvalid_1's l1: 0.116552\n",
      "[4200]\ttraining's l1: 0.112658\tvalid_1's l1: 0.116552\n",
      "[4300]\ttraining's l1: 0.112536\tvalid_1's l1: 0.116545\n",
      "[4400]\ttraining's l1: 0.112432\tvalid_1's l1: 0.116541\n",
      "[4500]\ttraining's l1: 0.112337\tvalid_1's l1: 0.116546\n",
      "Early stopping, best iteration is:\n",
      "[4353]\ttraining's l1: 0.112485\tvalid_1's l1: 0.11654\n",
      "fold = 2, score = 0.11653962341663306\n",
      "Fold-3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.632874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061922, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.132968\tvalid_1's l1: 0.13389\n",
      "[200]\ttraining's l1: 0.125865\tvalid_1's l1: 0.127226\n",
      "[300]\ttraining's l1: 0.123262\tvalid_1's l1: 0.124683\n",
      "[400]\ttraining's l1: 0.12179\tvalid_1's l1: 0.123311\n",
      "[500]\ttraining's l1: 0.120507\tvalid_1's l1: 0.122337\n",
      "[600]\ttraining's l1: 0.119574\tvalid_1's l1: 0.121787\n",
      "[700]\ttraining's l1: 0.118989\tvalid_1's l1: 0.121489\n",
      "[800]\ttraining's l1: 0.118473\tvalid_1's l1: 0.121269\n",
      "[900]\ttraining's l1: 0.118052\tvalid_1's l1: 0.121\n",
      "[1000]\ttraining's l1: 0.117652\tvalid_1's l1: 0.120836\n",
      "[1100]\ttraining's l1: 0.117394\tvalid_1's l1: 0.120755\n",
      "[1200]\ttraining's l1: 0.117127\tvalid_1's l1: 0.120675\n",
      "[1300]\ttraining's l1: 0.116883\tvalid_1's l1: 0.120565\n",
      "[1400]\ttraining's l1: 0.116617\tvalid_1's l1: 0.120468\n",
      "[1500]\ttraining's l1: 0.116397\tvalid_1's l1: 0.120348\n",
      "[1600]\ttraining's l1: 0.116186\tvalid_1's l1: 0.120262\n",
      "[1700]\ttraining's l1: 0.115975\tvalid_1's l1: 0.120182\n",
      "[1800]\ttraining's l1: 0.115806\tvalid_1's l1: 0.120115\n",
      "[1900]\ttraining's l1: 0.115681\tvalid_1's l1: 0.120094\n",
      "[2000]\ttraining's l1: 0.115558\tvalid_1's l1: 0.120066\n",
      "[2100]\ttraining's l1: 0.11539\tvalid_1's l1: 0.120042\n",
      "[2200]\ttraining's l1: 0.115241\tvalid_1's l1: 0.120001\n",
      "[2300]\ttraining's l1: 0.115065\tvalid_1's l1: 0.119957\n",
      "[2400]\ttraining's l1: 0.114901\tvalid_1's l1: 0.119894\n",
      "[2500]\ttraining's l1: 0.114762\tvalid_1's l1: 0.119872\n",
      "[2600]\ttraining's l1: 0.114605\tvalid_1's l1: 0.119844\n",
      "[2700]\ttraining's l1: 0.114469\tvalid_1's l1: 0.119826\n",
      "[2800]\ttraining's l1: 0.114352\tvalid_1's l1: 0.119804\n",
      "[2900]\ttraining's l1: 0.114213\tvalid_1's l1: 0.119758\n",
      "[3000]\ttraining's l1: 0.114101\tvalid_1's l1: 0.119738\n",
      "[3100]\ttraining's l1: 0.114004\tvalid_1's l1: 0.119703\n",
      "[3200]\ttraining's l1: 0.113887\tvalid_1's l1: 0.119673\n",
      "[3300]\ttraining's l1: 0.113776\tvalid_1's l1: 0.11965\n",
      "[3400]\ttraining's l1: 0.113668\tvalid_1's l1: 0.119629\n",
      "[3500]\ttraining's l1: 0.113575\tvalid_1's l1: 0.119608\n",
      "[3600]\ttraining's l1: 0.113468\tvalid_1's l1: 0.119583\n",
      "[3700]\ttraining's l1: 0.113335\tvalid_1's l1: 0.119546\n",
      "[3800]\ttraining's l1: 0.11323\tvalid_1's l1: 0.11952\n",
      "[3900]\ttraining's l1: 0.113136\tvalid_1's l1: 0.119499\n",
      "[4000]\ttraining's l1: 0.113041\tvalid_1's l1: 0.119487\n",
      "[4100]\ttraining's l1: 0.112929\tvalid_1's l1: 0.119459\n",
      "[4200]\ttraining's l1: 0.112856\tvalid_1's l1: 0.119455\n",
      "[4300]\ttraining's l1: 0.112779\tvalid_1's l1: 0.119443\n",
      "[4400]\ttraining's l1: 0.112696\tvalid_1's l1: 0.119429\n",
      "[4500]\ttraining's l1: 0.112614\tvalid_1's l1: 0.119427\n",
      "[4600]\ttraining's l1: 0.11254\tvalid_1's l1: 0.119413\n",
      "[4700]\ttraining's l1: 0.112413\tvalid_1's l1: 0.119388\n",
      "[4800]\ttraining's l1: 0.112328\tvalid_1's l1: 0.119385\n",
      "[4900]\ttraining's l1: 0.112262\tvalid_1's l1: 0.119379\n",
      "[5000]\ttraining's l1: 0.112183\tvalid_1's l1: 0.119373\n",
      "[5100]\ttraining's l1: 0.112073\tvalid_1's l1: 0.119377\n",
      "[5200]\ttraining's l1: 0.112013\tvalid_1's l1: 0.119375\n",
      "[5300]\ttraining's l1: 0.111923\tvalid_1's l1: 0.119354\n",
      "[5400]\ttraining's l1: 0.111851\tvalid_1's l1: 0.119348\n",
      "[5500]\ttraining's l1: 0.111798\tvalid_1's l1: 0.119349\n",
      "[5600]\ttraining's l1: 0.111713\tvalid_1's l1: 0.119334\n",
      "[5700]\ttraining's l1: 0.111652\tvalid_1's l1: 0.119329\n",
      "[5800]\ttraining's l1: 0.111577\tvalid_1's l1: 0.119307\n",
      "[5900]\ttraining's l1: 0.111513\tvalid_1's l1: 0.119304\n",
      "[6000]\ttraining's l1: 0.111431\tvalid_1's l1: 0.119295\n",
      "[6100]\ttraining's l1: 0.111357\tvalid_1's l1: 0.119289\n",
      "[6200]\ttraining's l1: 0.111292\tvalid_1's l1: 0.119286\n",
      "[6300]\ttraining's l1: 0.111229\tvalid_1's l1: 0.119286\n",
      "[6400]\ttraining's l1: 0.111171\tvalid_1's l1: 0.119284\n",
      "[6500]\ttraining's l1: 0.111102\tvalid_1's l1: 0.119287\n",
      "[6600]\ttraining's l1: 0.111021\tvalid_1's l1: 0.119283\n",
      "[6700]\ttraining's l1: 0.110965\tvalid_1's l1: 0.119281\n",
      "[6800]\ttraining's l1: 0.110913\tvalid_1's l1: 0.119276\n",
      "[6900]\ttraining's l1: 0.110843\tvalid_1's l1: 0.119277\n",
      "[7000]\ttraining's l1: 0.110767\tvalid_1's l1: 0.119279\n",
      "Early stopping, best iteration is:\n",
      "[6807]\ttraining's l1: 0.110909\tvalid_1's l1: 0.119274\n",
      "fold = 3, score = 0.11927360953371219\n",
      "Fold-4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.429941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061804, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.133511\tvalid_1's l1: 0.13342\n",
      "[200]\ttraining's l1: 0.125123\tvalid_1's l1: 0.125446\n",
      "[300]\ttraining's l1: 0.122879\tvalid_1's l1: 0.123308\n",
      "[400]\ttraining's l1: 0.121544\tvalid_1's l1: 0.12211\n",
      "[500]\ttraining's l1: 0.120293\tvalid_1's l1: 0.121146\n",
      "[600]\ttraining's l1: 0.119474\tvalid_1's l1: 0.120644\n",
      "[700]\ttraining's l1: 0.118865\tvalid_1's l1: 0.120246\n",
      "[800]\ttraining's l1: 0.118456\tvalid_1's l1: 0.120013\n",
      "[900]\ttraining's l1: 0.118019\tvalid_1's l1: 0.119715\n",
      "[1000]\ttraining's l1: 0.117644\tvalid_1's l1: 0.119525\n",
      "[1100]\ttraining's l1: 0.117335\tvalid_1's l1: 0.119357\n",
      "[1200]\ttraining's l1: 0.116992\tvalid_1's l1: 0.119212\n",
      "[1300]\ttraining's l1: 0.116753\tvalid_1's l1: 0.119096\n",
      "[1400]\ttraining's l1: 0.116506\tvalid_1's l1: 0.119049\n",
      "[1500]\ttraining's l1: 0.116288\tvalid_1's l1: 0.118986\n",
      "[1600]\ttraining's l1: 0.116071\tvalid_1's l1: 0.11894\n",
      "[1700]\ttraining's l1: 0.115859\tvalid_1's l1: 0.118903\n",
      "[1800]\ttraining's l1: 0.115681\tvalid_1's l1: 0.118856\n",
      "[1900]\ttraining's l1: 0.115479\tvalid_1's l1: 0.118792\n",
      "[2000]\ttraining's l1: 0.11531\tvalid_1's l1: 0.118738\n",
      "[2100]\ttraining's l1: 0.115128\tvalid_1's l1: 0.118704\n",
      "[2200]\ttraining's l1: 0.114984\tvalid_1's l1: 0.118651\n",
      "[2300]\ttraining's l1: 0.11486\tvalid_1's l1: 0.118629\n",
      "[2400]\ttraining's l1: 0.114709\tvalid_1's l1: 0.118609\n",
      "[2500]\ttraining's l1: 0.114576\tvalid_1's l1: 0.118588\n",
      "[2600]\ttraining's l1: 0.114443\tvalid_1's l1: 0.118568\n",
      "[2700]\ttraining's l1: 0.114312\tvalid_1's l1: 0.118544\n",
      "[2800]\ttraining's l1: 0.114181\tvalid_1's l1: 0.11853\n",
      "[2900]\ttraining's l1: 0.114057\tvalid_1's l1: 0.11851\n",
      "[3000]\ttraining's l1: 0.113945\tvalid_1's l1: 0.118494\n",
      "[3100]\ttraining's l1: 0.113818\tvalid_1's l1: 0.118469\n",
      "[3200]\ttraining's l1: 0.113724\tvalid_1's l1: 0.11845\n",
      "[3300]\ttraining's l1: 0.113634\tvalid_1's l1: 0.118437\n",
      "[3400]\ttraining's l1: 0.11356\tvalid_1's l1: 0.118419\n",
      "[3500]\ttraining's l1: 0.113449\tvalid_1's l1: 0.118404\n",
      "[3600]\ttraining's l1: 0.113345\tvalid_1's l1: 0.118394\n",
      "[3700]\ttraining's l1: 0.113256\tvalid_1's l1: 0.118393\n",
      "[3800]\ttraining's l1: 0.113134\tvalid_1's l1: 0.118378\n",
      "[3900]\ttraining's l1: 0.113038\tvalid_1's l1: 0.118373\n",
      "[4000]\ttraining's l1: 0.112939\tvalid_1's l1: 0.11836\n",
      "[4100]\ttraining's l1: 0.112852\tvalid_1's l1: 0.118347\n",
      "[4200]\ttraining's l1: 0.112758\tvalid_1's l1: 0.11834\n",
      "[4300]\ttraining's l1: 0.112674\tvalid_1's l1: 0.118338\n",
      "[4400]\ttraining's l1: 0.112587\tvalid_1's l1: 0.118322\n",
      "[4500]\ttraining's l1: 0.112485\tvalid_1's l1: 0.118323\n",
      "[4600]\ttraining's l1: 0.112394\tvalid_1's l1: 0.118316\n",
      "[4700]\ttraining's l1: 0.112288\tvalid_1's l1: 0.118318\n",
      "[4800]\ttraining's l1: 0.11221\tvalid_1's l1: 0.118302\n",
      "[4900]\ttraining's l1: 0.112121\tvalid_1's l1: 0.118302\n",
      "[5000]\ttraining's l1: 0.112051\tvalid_1's l1: 0.118293\n",
      "[5100]\ttraining's l1: 0.111958\tvalid_1's l1: 0.118284\n",
      "[5200]\ttraining's l1: 0.111885\tvalid_1's l1: 0.118286\n",
      "[5300]\ttraining's l1: 0.111815\tvalid_1's l1: 0.118281\n",
      "[5400]\ttraining's l1: 0.111748\tvalid_1's l1: 0.118275\n",
      "[5500]\ttraining's l1: 0.111675\tvalid_1's l1: 0.11827\n",
      "[5600]\ttraining's l1: 0.111603\tvalid_1's l1: 0.11827\n",
      "[5700]\ttraining's l1: 0.111541\tvalid_1's l1: 0.118263\n",
      "[5800]\ttraining's l1: 0.111471\tvalid_1's l1: 0.118251\n",
      "[5900]\ttraining's l1: 0.111395\tvalid_1's l1: 0.118243\n",
      "[6000]\ttraining's l1: 0.111324\tvalid_1's l1: 0.118235\n",
      "[6100]\ttraining's l1: 0.111245\tvalid_1's l1: 0.118229\n",
      "[6200]\ttraining's l1: 0.111187\tvalid_1's l1: 0.118225\n",
      "[6300]\ttraining's l1: 0.111122\tvalid_1's l1: 0.11823\n",
      "[6400]\ttraining's l1: 0.111064\tvalid_1's l1: 0.11823\n",
      "Early stopping, best iteration is:\n",
      "[6209]\ttraining's l1: 0.111182\tvalid_1's l1: 0.118224\n",
      "fold = 4, score = 0.11822431102648465\n",
      "Fold-5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.585058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061837, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.13382\tvalid_1's l1: 0.13545\n",
      "[200]\ttraining's l1: 0.126261\tvalid_1's l1: 0.127622\n",
      "[300]\ttraining's l1: 0.122799\tvalid_1's l1: 0.124149\n",
      "[400]\ttraining's l1: 0.121571\tvalid_1's l1: 0.123031\n",
      "[500]\ttraining's l1: 0.120414\tvalid_1's l1: 0.122265\n",
      "[600]\ttraining's l1: 0.119396\tvalid_1's l1: 0.121758\n",
      "[700]\ttraining's l1: 0.118794\tvalid_1's l1: 0.121499\n",
      "[800]\ttraining's l1: 0.118349\tvalid_1's l1: 0.121271\n",
      "[900]\ttraining's l1: 0.117943\tvalid_1's l1: 0.121084\n",
      "[1000]\ttraining's l1: 0.117627\tvalid_1's l1: 0.120952\n",
      "[1100]\ttraining's l1: 0.117331\tvalid_1's l1: 0.120859\n",
      "[1200]\ttraining's l1: 0.11704\tvalid_1's l1: 0.120744\n",
      "[1300]\ttraining's l1: 0.116779\tvalid_1's l1: 0.120651\n",
      "[1400]\ttraining's l1: 0.116552\tvalid_1's l1: 0.1206\n",
      "[1500]\ttraining's l1: 0.116332\tvalid_1's l1: 0.120521\n",
      "[1600]\ttraining's l1: 0.116021\tvalid_1's l1: 0.120373\n",
      "[1700]\ttraining's l1: 0.115793\tvalid_1's l1: 0.120263\n",
      "[1800]\ttraining's l1: 0.115588\tvalid_1's l1: 0.120181\n",
      "[1900]\ttraining's l1: 0.115384\tvalid_1's l1: 0.12011\n",
      "[2000]\ttraining's l1: 0.115194\tvalid_1's l1: 0.120062\n",
      "[2100]\ttraining's l1: 0.115031\tvalid_1's l1: 0.120008\n",
      "[2200]\ttraining's l1: 0.114869\tvalid_1's l1: 0.119967\n",
      "[2300]\ttraining's l1: 0.114658\tvalid_1's l1: 0.119854\n",
      "[2400]\ttraining's l1: 0.11449\tvalid_1's l1: 0.119805\n",
      "[2500]\ttraining's l1: 0.114317\tvalid_1's l1: 0.119727\n",
      "[2600]\ttraining's l1: 0.114157\tvalid_1's l1: 0.119674\n",
      "[2700]\ttraining's l1: 0.114037\tvalid_1's l1: 0.119658\n",
      "[2800]\ttraining's l1: 0.113919\tvalid_1's l1: 0.119627\n",
      "[2900]\ttraining's l1: 0.113748\tvalid_1's l1: 0.119577\n",
      "[3000]\ttraining's l1: 0.113644\tvalid_1's l1: 0.119562\n",
      "[3100]\ttraining's l1: 0.113557\tvalid_1's l1: 0.119552\n",
      "[3200]\ttraining's l1: 0.113448\tvalid_1's l1: 0.119523\n",
      "[3300]\ttraining's l1: 0.113319\tvalid_1's l1: 0.119496\n",
      "[3400]\ttraining's l1: 0.11322\tvalid_1's l1: 0.119477\n",
      "[3500]\ttraining's l1: 0.11312\tvalid_1's l1: 0.119471\n",
      "[3600]\ttraining's l1: 0.113035\tvalid_1's l1: 0.119457\n",
      "[3700]\ttraining's l1: 0.112936\tvalid_1's l1: 0.119443\n",
      "[3800]\ttraining's l1: 0.112818\tvalid_1's l1: 0.119428\n",
      "[3900]\ttraining's l1: 0.112732\tvalid_1's l1: 0.11941\n",
      "[4000]\ttraining's l1: 0.112645\tvalid_1's l1: 0.119398\n",
      "[4100]\ttraining's l1: 0.112552\tvalid_1's l1: 0.119388\n",
      "[4200]\ttraining's l1: 0.112476\tvalid_1's l1: 0.119384\n",
      "[4300]\ttraining's l1: 0.112397\tvalid_1's l1: 0.119372\n",
      "[4400]\ttraining's l1: 0.112316\tvalid_1's l1: 0.119369\n",
      "[4500]\ttraining's l1: 0.11224\tvalid_1's l1: 0.119364\n",
      "[4600]\ttraining's l1: 0.112145\tvalid_1's l1: 0.119354\n",
      "[4700]\ttraining's l1: 0.11206\tvalid_1's l1: 0.119341\n",
      "[4800]\ttraining's l1: 0.111985\tvalid_1's l1: 0.119329\n",
      "[4900]\ttraining's l1: 0.111892\tvalid_1's l1: 0.119309\n",
      "[5000]\ttraining's l1: 0.111823\tvalid_1's l1: 0.119302\n",
      "[5100]\ttraining's l1: 0.111751\tvalid_1's l1: 0.119298\n",
      "[5200]\ttraining's l1: 0.111653\tvalid_1's l1: 0.119286\n",
      "[5300]\ttraining's l1: 0.111569\tvalid_1's l1: 0.119275\n",
      "[5400]\ttraining's l1: 0.111477\tvalid_1's l1: 0.119267\n",
      "[5500]\ttraining's l1: 0.111412\tvalid_1's l1: 0.119264\n",
      "[5600]\ttraining's l1: 0.111331\tvalid_1's l1: 0.119258\n",
      "[5700]\ttraining's l1: 0.111246\tvalid_1's l1: 0.119252\n",
      "[5800]\ttraining's l1: 0.111175\tvalid_1's l1: 0.119247\n",
      "[5900]\ttraining's l1: 0.111111\tvalid_1's l1: 0.119243\n",
      "[6000]\ttraining's l1: 0.111036\tvalid_1's l1: 0.119236\n",
      "[6100]\ttraining's l1: 0.110967\tvalid_1's l1: 0.11924\n",
      "[6200]\ttraining's l1: 0.11091\tvalid_1's l1: 0.119241\n",
      "Early stopping, best iteration is:\n",
      "[6020]\ttraining's l1: 0.111021\tvalid_1's l1: 0.119233\n",
      "fold = 5, score = 0.11923326813859873\n",
      "Fold-6\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.617304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061818, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.13253\tvalid_1's l1: 0.131004\n",
      "[200]\ttraining's l1: 0.125815\tvalid_1's l1: 0.125164\n",
      "[300]\ttraining's l1: 0.122867\tvalid_1's l1: 0.122548\n",
      "[400]\ttraining's l1: 0.121296\tvalid_1's l1: 0.121186\n",
      "[500]\ttraining's l1: 0.120206\tvalid_1's l1: 0.120274\n",
      "[600]\ttraining's l1: 0.119416\tvalid_1's l1: 0.119901\n",
      "[700]\ttraining's l1: 0.118852\tvalid_1's l1: 0.119685\n",
      "[800]\ttraining's l1: 0.118447\tvalid_1's l1: 0.119505\n",
      "[900]\ttraining's l1: 0.117986\tvalid_1's l1: 0.119356\n",
      "[1000]\ttraining's l1: 0.117577\tvalid_1's l1: 0.119199\n",
      "[1100]\ttraining's l1: 0.117233\tvalid_1's l1: 0.119103\n",
      "[1200]\ttraining's l1: 0.116976\tvalid_1's l1: 0.11904\n",
      "[1300]\ttraining's l1: 0.116727\tvalid_1's l1: 0.118963\n",
      "[1400]\ttraining's l1: 0.116514\tvalid_1's l1: 0.11892\n",
      "[1500]\ttraining's l1: 0.116325\tvalid_1's l1: 0.118862\n",
      "[1600]\ttraining's l1: 0.116102\tvalid_1's l1: 0.118805\n",
      "[1700]\ttraining's l1: 0.115896\tvalid_1's l1: 0.118759\n",
      "[1800]\ttraining's l1: 0.115699\tvalid_1's l1: 0.118734\n",
      "[1900]\ttraining's l1: 0.115531\tvalid_1's l1: 0.118719\n",
      "[2000]\ttraining's l1: 0.115356\tvalid_1's l1: 0.118669\n",
      "[2100]\ttraining's l1: 0.115207\tvalid_1's l1: 0.118653\n",
      "[2200]\ttraining's l1: 0.115065\tvalid_1's l1: 0.118615\n",
      "[2300]\ttraining's l1: 0.114892\tvalid_1's l1: 0.118557\n",
      "[2400]\ttraining's l1: 0.11475\tvalid_1's l1: 0.118527\n",
      "[2500]\ttraining's l1: 0.114604\tvalid_1's l1: 0.118509\n",
      "[2600]\ttraining's l1: 0.114477\tvalid_1's l1: 0.118506\n",
      "[2700]\ttraining's l1: 0.114321\tvalid_1's l1: 0.118483\n",
      "[2800]\ttraining's l1: 0.114212\tvalid_1's l1: 0.118469\n",
      "[2900]\ttraining's l1: 0.114069\tvalid_1's l1: 0.118438\n",
      "[3000]\ttraining's l1: 0.11395\tvalid_1's l1: 0.118422\n",
      "[3100]\ttraining's l1: 0.113836\tvalid_1's l1: 0.118407\n",
      "[3200]\ttraining's l1: 0.113737\tvalid_1's l1: 0.118401\n",
      "[3300]\ttraining's l1: 0.113626\tvalid_1's l1: 0.118392\n",
      "[3400]\ttraining's l1: 0.113511\tvalid_1's l1: 0.118385\n",
      "[3500]\ttraining's l1: 0.113417\tvalid_1's l1: 0.118379\n",
      "[3600]\ttraining's l1: 0.113303\tvalid_1's l1: 0.118353\n",
      "[3700]\ttraining's l1: 0.113187\tvalid_1's l1: 0.118336\n",
      "[3800]\ttraining's l1: 0.113081\tvalid_1's l1: 0.118317\n",
      "[3900]\ttraining's l1: 0.112996\tvalid_1's l1: 0.118314\n",
      "[4000]\ttraining's l1: 0.1129\tvalid_1's l1: 0.118308\n",
      "[4100]\ttraining's l1: 0.112796\tvalid_1's l1: 0.118292\n",
      "[4200]\ttraining's l1: 0.112713\tvalid_1's l1: 0.118282\n",
      "[4300]\ttraining's l1: 0.112644\tvalid_1's l1: 0.118275\n",
      "[4400]\ttraining's l1: 0.112547\tvalid_1's l1: 0.118272\n",
      "[4500]\ttraining's l1: 0.112469\tvalid_1's l1: 0.118266\n",
      "[4600]\ttraining's l1: 0.112396\tvalid_1's l1: 0.118263\n",
      "[4700]\ttraining's l1: 0.112331\tvalid_1's l1: 0.118263\n",
      "[4800]\ttraining's l1: 0.112255\tvalid_1's l1: 0.118253\n",
      "[4900]\ttraining's l1: 0.11219\tvalid_1's l1: 0.118245\n",
      "[5000]\ttraining's l1: 0.1121\tvalid_1's l1: 0.118234\n",
      "[5100]\ttraining's l1: 0.112028\tvalid_1's l1: 0.118225\n",
      "[5200]\ttraining's l1: 0.111948\tvalid_1's l1: 0.118211\n",
      "[5300]\ttraining's l1: 0.111857\tvalid_1's l1: 0.118209\n",
      "[5400]\ttraining's l1: 0.11179\tvalid_1's l1: 0.118213\n",
      "Early stopping, best iteration is:\n",
      "[5221]\ttraining's l1: 0.111928\tvalid_1's l1: 0.118208\n",
      "fold = 6, score = 0.11820808738539829\n",
      "Fold-7\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.590554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061785, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.132618\tvalid_1's l1: 0.135551\n",
      "[200]\ttraining's l1: 0.125176\tvalid_1's l1: 0.128187\n",
      "[300]\ttraining's l1: 0.122994\tvalid_1's l1: 0.126102\n",
      "[400]\ttraining's l1: 0.121036\tvalid_1's l1: 0.124433\n",
      "[500]\ttraining's l1: 0.119996\tvalid_1's l1: 0.123657\n",
      "[600]\ttraining's l1: 0.119166\tvalid_1's l1: 0.123152\n",
      "[700]\ttraining's l1: 0.118625\tvalid_1's l1: 0.122835\n",
      "[800]\ttraining's l1: 0.118046\tvalid_1's l1: 0.122474\n",
      "[900]\ttraining's l1: 0.117578\tvalid_1's l1: 0.122259\n",
      "[1000]\ttraining's l1: 0.117212\tvalid_1's l1: 0.122139\n",
      "[1100]\ttraining's l1: 0.116892\tvalid_1's l1: 0.122031\n",
      "[1200]\ttraining's l1: 0.116601\tvalid_1's l1: 0.121936\n",
      "[1300]\ttraining's l1: 0.116317\tvalid_1's l1: 0.121852\n",
      "[1400]\ttraining's l1: 0.116082\tvalid_1's l1: 0.121767\n",
      "[1500]\ttraining's l1: 0.115903\tvalid_1's l1: 0.121709\n",
      "[1600]\ttraining's l1: 0.115684\tvalid_1's l1: 0.121619\n",
      "[1700]\ttraining's l1: 0.115501\tvalid_1's l1: 0.121573\n",
      "[1800]\ttraining's l1: 0.115289\tvalid_1's l1: 0.121528\n",
      "[1900]\ttraining's l1: 0.115113\tvalid_1's l1: 0.121494\n",
      "[2000]\ttraining's l1: 0.114965\tvalid_1's l1: 0.121473\n",
      "[2100]\ttraining's l1: 0.114796\tvalid_1's l1: 0.121435\n",
      "[2200]\ttraining's l1: 0.114642\tvalid_1's l1: 0.121412\n",
      "[2300]\ttraining's l1: 0.114505\tvalid_1's l1: 0.121355\n",
      "[2400]\ttraining's l1: 0.114333\tvalid_1's l1: 0.1213\n",
      "[2500]\ttraining's l1: 0.11421\tvalid_1's l1: 0.121271\n",
      "[2600]\ttraining's l1: 0.114073\tvalid_1's l1: 0.121239\n",
      "[2700]\ttraining's l1: 0.113956\tvalid_1's l1: 0.121222\n",
      "[2800]\ttraining's l1: 0.113823\tvalid_1's l1: 0.121185\n",
      "[2900]\ttraining's l1: 0.113715\tvalid_1's l1: 0.121153\n",
      "[3000]\ttraining's l1: 0.113597\tvalid_1's l1: 0.12113\n",
      "[3100]\ttraining's l1: 0.113494\tvalid_1's l1: 0.121099\n",
      "[3200]\ttraining's l1: 0.113376\tvalid_1's l1: 0.121082\n",
      "[3300]\ttraining's l1: 0.113236\tvalid_1's l1: 0.121047\n",
      "[3400]\ttraining's l1: 0.113146\tvalid_1's l1: 0.121034\n",
      "[3500]\ttraining's l1: 0.113044\tvalid_1's l1: 0.121027\n",
      "[3600]\ttraining's l1: 0.112945\tvalid_1's l1: 0.121014\n",
      "[3700]\ttraining's l1: 0.112861\tvalid_1's l1: 0.121008\n",
      "[3800]\ttraining's l1: 0.112758\tvalid_1's l1: 0.12099\n",
      "[3900]\ttraining's l1: 0.112661\tvalid_1's l1: 0.120976\n",
      "[4000]\ttraining's l1: 0.112562\tvalid_1's l1: 0.12096\n",
      "[4100]\ttraining's l1: 0.11245\tvalid_1's l1: 0.12095\n",
      "[4200]\ttraining's l1: 0.112339\tvalid_1's l1: 0.120936\n",
      "[4300]\ttraining's l1: 0.112217\tvalid_1's l1: 0.120912\n",
      "[4400]\ttraining's l1: 0.112121\tvalid_1's l1: 0.120896\n",
      "[4500]\ttraining's l1: 0.112034\tvalid_1's l1: 0.120879\n",
      "[4600]\ttraining's l1: 0.11193\tvalid_1's l1: 0.120855\n",
      "[4700]\ttraining's l1: 0.111841\tvalid_1's l1: 0.120838\n",
      "[4800]\ttraining's l1: 0.11174\tvalid_1's l1: 0.120812\n",
      "[4900]\ttraining's l1: 0.111664\tvalid_1's l1: 0.120801\n",
      "[5000]\ttraining's l1: 0.1116\tvalid_1's l1: 0.120792\n",
      "[5100]\ttraining's l1: 0.111519\tvalid_1's l1: 0.120782\n",
      "[5200]\ttraining's l1: 0.11146\tvalid_1's l1: 0.120783\n",
      "[5300]\ttraining's l1: 0.11137\tvalid_1's l1: 0.120781\n",
      "[5400]\ttraining's l1: 0.111297\tvalid_1's l1: 0.120773\n",
      "[5500]\ttraining's l1: 0.111234\tvalid_1's l1: 0.120766\n",
      "[5600]\ttraining's l1: 0.111162\tvalid_1's l1: 0.120766\n",
      "[5700]\ttraining's l1: 0.111089\tvalid_1's l1: 0.12077\n",
      "Early stopping, best iteration is:\n",
      "[5558]\ttraining's l1: 0.111198\tvalid_1's l1: 0.120763\n",
      "fold = 7, score = 0.12076344845809907\n",
      "Fold-8\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.609756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061827, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.890698\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.133841\tvalid_1's l1: 0.134959\n",
      "[200]\ttraining's l1: 0.126468\tvalid_1's l1: 0.127191\n",
      "[300]\ttraining's l1: 0.123653\tvalid_1's l1: 0.12449\n",
      "[400]\ttraining's l1: 0.121671\tvalid_1's l1: 0.122684\n",
      "[500]\ttraining's l1: 0.120521\tvalid_1's l1: 0.121849\n",
      "[600]\ttraining's l1: 0.119686\tvalid_1's l1: 0.12127\n",
      "[700]\ttraining's l1: 0.119112\tvalid_1's l1: 0.121042\n",
      "[800]\ttraining's l1: 0.118584\tvalid_1's l1: 0.120689\n",
      "[900]\ttraining's l1: 0.118117\tvalid_1's l1: 0.120466\n",
      "[1000]\ttraining's l1: 0.117708\tvalid_1's l1: 0.120235\n",
      "[1100]\ttraining's l1: 0.117434\tvalid_1's l1: 0.120099\n",
      "[1200]\ttraining's l1: 0.11715\tvalid_1's l1: 0.119987\n",
      "[1300]\ttraining's l1: 0.116908\tvalid_1's l1: 0.119912\n",
      "[1400]\ttraining's l1: 0.116673\tvalid_1's l1: 0.119863\n",
      "[1500]\ttraining's l1: 0.116471\tvalid_1's l1: 0.119827\n",
      "[1600]\ttraining's l1: 0.116188\tvalid_1's l1: 0.119686\n",
      "[1700]\ttraining's l1: 0.115988\tvalid_1's l1: 0.119638\n",
      "[1800]\ttraining's l1: 0.115808\tvalid_1's l1: 0.119596\n",
      "[1900]\ttraining's l1: 0.115608\tvalid_1's l1: 0.119543\n",
      "[2000]\ttraining's l1: 0.115422\tvalid_1's l1: 0.119508\n",
      "[2100]\ttraining's l1: 0.115237\tvalid_1's l1: 0.119471\n",
      "[2200]\ttraining's l1: 0.115073\tvalid_1's l1: 0.119436\n",
      "[2300]\ttraining's l1: 0.114908\tvalid_1's l1: 0.119426\n",
      "[2400]\ttraining's l1: 0.114737\tvalid_1's l1: 0.119388\n",
      "[2500]\ttraining's l1: 0.114609\tvalid_1's l1: 0.119377\n",
      "[2600]\ttraining's l1: 0.114452\tvalid_1's l1: 0.119356\n",
      "[2700]\ttraining's l1: 0.114325\tvalid_1's l1: 0.119337\n",
      "[2800]\ttraining's l1: 0.114201\tvalid_1's l1: 0.119302\n",
      "[2900]\ttraining's l1: 0.114087\tvalid_1's l1: 0.119281\n",
      "[3000]\ttraining's l1: 0.11397\tvalid_1's l1: 0.119263\n",
      "[3100]\ttraining's l1: 0.113853\tvalid_1's l1: 0.119262\n",
      "[3200]\ttraining's l1: 0.113739\tvalid_1's l1: 0.119246\n",
      "[3300]\ttraining's l1: 0.11364\tvalid_1's l1: 0.119228\n",
      "[3400]\ttraining's l1: 0.113541\tvalid_1's l1: 0.119219\n",
      "[3500]\ttraining's l1: 0.113435\tvalid_1's l1: 0.119205\n",
      "[3600]\ttraining's l1: 0.11333\tvalid_1's l1: 0.119187\n",
      "[3700]\ttraining's l1: 0.113217\tvalid_1's l1: 0.119172\n",
      "[3800]\ttraining's l1: 0.113116\tvalid_1's l1: 0.119165\n",
      "[3900]\ttraining's l1: 0.113026\tvalid_1's l1: 0.11916\n",
      "[4000]\ttraining's l1: 0.112922\tvalid_1's l1: 0.119148\n",
      "[4100]\ttraining's l1: 0.112845\tvalid_1's l1: 0.119147\n",
      "[4200]\ttraining's l1: 0.112759\tvalid_1's l1: 0.119137\n",
      "[4300]\ttraining's l1: 0.112669\tvalid_1's l1: 0.119121\n",
      "[4400]\ttraining's l1: 0.112597\tvalid_1's l1: 0.119116\n",
      "[4500]\ttraining's l1: 0.112524\tvalid_1's l1: 0.119107\n",
      "[4600]\ttraining's l1: 0.112459\tvalid_1's l1: 0.119097\n",
      "[4700]\ttraining's l1: 0.112383\tvalid_1's l1: 0.119105\n",
      "[4800]\ttraining's l1: 0.112301\tvalid_1's l1: 0.119089\n",
      "[4900]\ttraining's l1: 0.112226\tvalid_1's l1: 0.119081\n",
      "[5000]\ttraining's l1: 0.112133\tvalid_1's l1: 0.11907\n",
      "[5100]\ttraining's l1: 0.112064\tvalid_1's l1: 0.119053\n",
      "[5200]\ttraining's l1: 0.111979\tvalid_1's l1: 0.119055\n",
      "[5300]\ttraining's l1: 0.111903\tvalid_1's l1: 0.119057\n",
      "[5400]\ttraining's l1: 0.111843\tvalid_1's l1: 0.119041\n",
      "[5500]\ttraining's l1: 0.111777\tvalid_1's l1: 0.119034\n",
      "[5600]\ttraining's l1: 0.111721\tvalid_1's l1: 0.119033\n",
      "[5700]\ttraining's l1: 0.111662\tvalid_1's l1: 0.119031\n",
      "[5800]\ttraining's l1: 0.111598\tvalid_1's l1: 0.119028\n",
      "[5900]\ttraining's l1: 0.111535\tvalid_1's l1: 0.11903\n",
      "[6000]\ttraining's l1: 0.111465\tvalid_1's l1: 0.119018\n",
      "[6100]\ttraining's l1: 0.111393\tvalid_1's l1: 0.119016\n",
      "[6200]\ttraining's l1: 0.111321\tvalid_1's l1: 0.119009\n",
      "[6300]\ttraining's l1: 0.111296\tvalid_1's l1: 0.11901\n",
      "Early stopping, best iteration is:\n",
      "[6178]\ttraining's l1: 0.111334\tvalid_1's l1: 0.119006\n",
      "fold = 8, score = 0.1190061399734569\n",
      "Fold-9\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.598432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 787648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2061896, number of used features: 205\n",
      "[LightGBM] [Info] Start training from score 15.820396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's l1: 0.133519\tvalid_1's l1: 0.135079\n",
      "[200]\ttraining's l1: 0.125808\tvalid_1's l1: 0.126994\n",
      "[300]\ttraining's l1: 0.123138\tvalid_1's l1: 0.124398\n",
      "[400]\ttraining's l1: 0.121425\tvalid_1's l1: 0.122967\n",
      "[500]\ttraining's l1: 0.120317\tvalid_1's l1: 0.122231\n",
      "[600]\ttraining's l1: 0.119526\tvalid_1's l1: 0.121667\n",
      "[700]\ttraining's l1: 0.119041\tvalid_1's l1: 0.12142\n",
      "[800]\ttraining's l1: 0.118523\tvalid_1's l1: 0.121141\n",
      "[900]\ttraining's l1: 0.117997\tvalid_1's l1: 0.12082\n",
      "[1000]\ttraining's l1: 0.117626\tvalid_1's l1: 0.120574\n",
      "[1100]\ttraining's l1: 0.11733\tvalid_1's l1: 0.120461\n",
      "[1200]\ttraining's l1: 0.116988\tvalid_1's l1: 0.120257\n",
      "[1300]\ttraining's l1: 0.116795\tvalid_1's l1: 0.120159\n",
      "[1400]\ttraining's l1: 0.116561\tvalid_1's l1: 0.120051\n",
      "[1500]\ttraining's l1: 0.116363\tvalid_1's l1: 0.11999\n",
      "[1600]\ttraining's l1: 0.116191\tvalid_1's l1: 0.119913\n",
      "[1700]\ttraining's l1: 0.115958\tvalid_1's l1: 0.119832\n",
      "[1800]\ttraining's l1: 0.115772\tvalid_1's l1: 0.119756\n",
      "[1900]\ttraining's l1: 0.115592\tvalid_1's l1: 0.119675\n",
      "[2000]\ttraining's l1: 0.115403\tvalid_1's l1: 0.119632\n",
      "[2100]\ttraining's l1: 0.115232\tvalid_1's l1: 0.119591\n",
      "[2200]\ttraining's l1: 0.115067\tvalid_1's l1: 0.119558\n",
      "[2300]\ttraining's l1: 0.114869\tvalid_1's l1: 0.119485\n",
      "[2400]\ttraining's l1: 0.114671\tvalid_1's l1: 0.119445\n",
      "[2500]\ttraining's l1: 0.114527\tvalid_1's l1: 0.119411\n",
      "[2600]\ttraining's l1: 0.114389\tvalid_1's l1: 0.119385\n",
      "[2700]\ttraining's l1: 0.114264\tvalid_1's l1: 0.119359\n",
      "[2800]\ttraining's l1: 0.114125\tvalid_1's l1: 0.119345\n",
      "[2900]\ttraining's l1: 0.113983\tvalid_1's l1: 0.11931\n",
      "[3000]\ttraining's l1: 0.113834\tvalid_1's l1: 0.119277\n",
      "[3100]\ttraining's l1: 0.113731\tvalid_1's l1: 0.119266\n",
      "[3200]\ttraining's l1: 0.113619\tvalid_1's l1: 0.119246\n",
      "[3300]\ttraining's l1: 0.113499\tvalid_1's l1: 0.119221\n",
      "[3400]\ttraining's l1: 0.113394\tvalid_1's l1: 0.119195\n",
      "[3500]\ttraining's l1: 0.113293\tvalid_1's l1: 0.119172\n",
      "[3600]\ttraining's l1: 0.11319\tvalid_1's l1: 0.119154\n",
      "[3700]\ttraining's l1: 0.113095\tvalid_1's l1: 0.119141\n",
      "[3800]\ttraining's l1: 0.113005\tvalid_1's l1: 0.119122\n",
      "[3900]\ttraining's l1: 0.112916\tvalid_1's l1: 0.119101\n",
      "[4000]\ttraining's l1: 0.112819\tvalid_1's l1: 0.119081\n",
      "[4100]\ttraining's l1: 0.1127\tvalid_1's l1: 0.119058\n",
      "[4200]\ttraining's l1: 0.112582\tvalid_1's l1: 0.11905\n",
      "[4300]\ttraining's l1: 0.112471\tvalid_1's l1: 0.119038\n",
      "[4400]\ttraining's l1: 0.112383\tvalid_1's l1: 0.119029\n",
      "[4500]\ttraining's l1: 0.112305\tvalid_1's l1: 0.119021\n",
      "[4600]\ttraining's l1: 0.112213\tvalid_1's l1: 0.119004\n",
      "[4700]\ttraining's l1: 0.112134\tvalid_1's l1: 0.119001\n",
      "[4800]\ttraining's l1: 0.112045\tvalid_1's l1: 0.118992\n",
      "[4900]\ttraining's l1: 0.111953\tvalid_1's l1: 0.118981\n",
      "[5000]\ttraining's l1: 0.111866\tvalid_1's l1: 0.118974\n",
      "[5100]\ttraining's l1: 0.111783\tvalid_1's l1: 0.118955\n",
      "[5200]\ttraining's l1: 0.111674\tvalid_1's l1: 0.118944\n",
      "[5300]\ttraining's l1: 0.111585\tvalid_1's l1: 0.118928\n",
      "[5400]\ttraining's l1: 0.111498\tvalid_1's l1: 0.118911\n",
      "[5500]\ttraining's l1: 0.111403\tvalid_1's l1: 0.118886\n",
      "[5600]\ttraining's l1: 0.111337\tvalid_1's l1: 0.118875\n",
      "[5700]\ttraining's l1: 0.111249\tvalid_1's l1: 0.11886\n",
      "[5800]\ttraining's l1: 0.111184\tvalid_1's l1: 0.118847\n",
      "[5900]\ttraining's l1: 0.111123\tvalid_1's l1: 0.118842\n",
      "[6000]\ttraining's l1: 0.111042\tvalid_1's l1: 0.118845\n",
      "[6100]\ttraining's l1: 0.110968\tvalid_1's l1: 0.118831\n",
      "[6200]\ttraining's l1: 0.110893\tvalid_1's l1: 0.118824\n",
      "[6300]\ttraining's l1: 0.110832\tvalid_1's l1: 0.118813\n",
      "[6400]\ttraining's l1: 0.110748\tvalid_1's l1: 0.118813\n",
      "[6500]\ttraining's l1: 0.110689\tvalid_1's l1: 0.118801\n",
      "[6600]\ttraining's l1: 0.110623\tvalid_1's l1: 0.118793\n",
      "[6700]\ttraining's l1: 0.110564\tvalid_1's l1: 0.118793\n",
      "Early stopping, best iteration is:\n",
      "[6580]\ttraining's l1: 0.110639\tvalid_1's l1: 0.118791\n",
      "fold = 9, score = 0.11879120790002501\n"
     ]
    }
   ],
   "source": [
    "oof_prediction = np.zeros(len(train_df))\n",
    "test_preds_lst = []\n",
    "input_dim = len(train_value_col)\n",
    "train_df['pred'] = 0\n",
    "train_gby = train_df.groupby('breath_id')['R_C'].agg('first').reset_index()\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "models = []\n",
    "\n",
    "\n",
    "fold_df = pd.DataFrame()\n",
    "fold_df[\"id\"] = train[\"id\"]\n",
    "fold_df[\"fold\"] = -1\n",
    "\n",
    "for fold, (_, valid_idx) in enumerate(skf.split(train_gby, train_gby['R_C'])):        \n",
    "    valid_b_ids = train_gby.iloc[valid_idx]['breath_id'].values\n",
    "    valid_df_idx = train_df[train_df['breath_id'].isin(valid_b_ids)].index.to_list()\n",
    "    fold_df.loc[valid_df_idx, 'fold'] = fold\n",
    "\n",
    "for i, fold in enumerate(range(CFG.n_folds)):\n",
    "    if i not in CFG.folds:\n",
    "        continue\n",
    "    print(f'Fold-{fold}')\n",
    "    \n",
    "    train_idx = fold_df[fold_df[\"fold\"] != fold].index\n",
    "    valid_idx = fold_df[fold_df[\"fold\"] == fold].index\n",
    "    \n",
    "    trn_df = train_df.loc[fold_df[\"fold\"] != fold, train_value_col].reset_index(drop=True)\n",
    "    val_df = train_df.loc[fold_df[\"fold\"] == fold, train_value_col].reset_index(drop=True)\n",
    "    trn_y = train_df.loc[fold_df[\"fold\"] != fold, 'pressure'].reset_index(drop=True)\n",
    "    val_y = train_df.loc[fold_df[\"fold\"] == fold, 'pressure'].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    lgb_train = lgb.Dataset(trn_df, trn_y)\n",
    "    lgb_valid = lgb.Dataset(val_df, val_y, reference=lgb_train)\n",
    "\n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "             'objective': 'l1',\n",
    "            'metric': 'l1',\n",
    "            'seed': CFG.seed,\n",
    "            'max_bin': 3880, \n",
    "            'min_data_in_leaf': 1217, \n",
    "            'num_leaves': 36,\n",
    "            'num_boost_round': 10000,\n",
    "            'learning_rate': 0.1,\n",
    "            'lambda_l1': 0.1,\n",
    "            'lambda_l2': 0.1,\n",
    "    }\n",
    "\n",
    "\n",
    "    model = lgb.train(params, \n",
    "                      train_set=lgb_train, \n",
    "                      valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=200, verbose_eval=100)\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    oof_prediction[valid_idx] = model.predict(val_df[train_value_col])\n",
    "    test_pred = model.predict(test_df[train_value_col])\n",
    "    test_preds_lst.append(test_pred)\n",
    "    score = np.abs(val_y.values - oof_prediction[valid_idx]).mean()\n",
    "    print(f'fold = {fold}, score = {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11870159546646876"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = np.abs(y.values - oof_prediction).mean()\n",
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(OUTPUT_DIR / f\"stacking_oof_{CFG.exp_num}\", oof_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pressures = train[\"pressure\"].unique()\n",
    "sorted_pressures = np.sort(unique_pressures)\n",
    "total_pressures_len = len(sorted_pressures)\n",
    "\n",
    "def find_nearest(prediction):\n",
    "    insert_idx = np.searchsorted(sorted_pressures, prediction)\n",
    "    if insert_idx == total_pressures_len:\n",
    "        # If the predicted value is bigger than the highest pressure in the train dataset,\n",
    "        # return the max value.\n",
    "        return sorted_pressures[-1]\n",
    "    elif insert_idx == 0:\n",
    "        # Same control but for the lower bound.\n",
    "        return sorted_pressures[0]\n",
    "    lower_val = sorted_pressures[insert_idx - 1]\n",
    "    upper_val = sorted_pressures[insert_idx]\n",
    "    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11722455779867644\n"
     ]
    }
   ],
   "source": [
    "oof = pd.DataFrame({'pred': oof_prediction})\n",
    "oof_pp = oof['pred'].map(lambda x: unique_pressures[np.abs(unique_pressures-x).argmin()])\n",
    "score = np.abs(y.values - oof_pp).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "\n",
    "sub_df.loc[test['u_out']==0, 'pressure'] = np.stack(test_preds_lst).mean(0)\n",
    "sub_df.to_csv(OUTPUT_DIR / f\"stacking_submission_mean_{CFG.exp_num}.csv\", index=None)\n",
    "\n",
    "sub_df.loc[test['u_out']==0, 'pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n",
    "sub_df.to_csv(OUTPUT_DIR / f\"stacking_submission_median_{CFG.exp_num}.csv\", index=None)\n",
    "\n",
    "# Post Processing: https://www.kaggle.com/snnclsr/a-dummy-approach-to-improve-your-score-postprocess\n",
    "\n",
    "\n",
    "sub_df = pd.read_csv(OUTPUT_DIR / f\"stacking_submission_mean_{CFG.exp_num}.csv\")\n",
    "sub_df.loc[test['u_out']==0, 'pressure'] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "sub_df.to_csv(OUTPUT_DIR / f\"stacking_submission_mean_pp_{CFG.exp_num}.csv\", index=None)\n",
    "\n",
    "sub_df = pd.read_csv(OUTPUT_DIR / f\"stacking_submission_median_{CFG.exp_num}.csv\")\n",
    "sub_df.loc[test['u_out']==0, 'pressure'] = sub_df[\"pressure\"].apply(find_nearest)\n",
    "sub_df.to_csv(OUTPUT_DIR / f\"stacking_submission_median_pp_{CFG.exp_num}.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5cb25c469b873a0e0eec115bfbbdb6f77ff8970c2724434dd7f3f7fbd6e0533"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
